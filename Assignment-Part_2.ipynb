{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f18240-073a-46be-a24a-6f02a0362d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MILESTONE ASSIGNMENT PART-2 \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7ded8-3442-417e-a578-0f19c1dc5e73",
   "metadata": {},
   "source": [
    "# 20. What do you mean by Measure of Central Tendency and Measures of Dispersion .How it can be calculated.\n",
    "\n",
    "###### Measures of Central Tendency:\n",
    "\n",
    "* Mean: Average of values.\n",
    "* Median: Middle value in sorted data.\n",
    "* Mode: Most frequent value.\n",
    "###### Measures of Dispersion:\n",
    "\n",
    "* Range: Difference between max and min values.\n",
    "* Variance: Average squared deviation from the mean.\n",
    "* Standard Deviation: Square root of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf9c7e-02ce-4217-8579-79a197c5c96c",
   "metadata": {},
   "source": [
    "# 21. What do you mean by skewness.Explain its types.Use graph to show.\n",
    "\n",
    "* Skewness measures the asymmetry of a dataset's distribution around its mean.\n",
    "* It indicates whether the data is skewed to the left or right, affecting the shape of the distribution curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2beaf7a-1e2b-495a-a5e8-efeb32f2bfe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate data for different types of skewness\n",
    "np.random.seed(0)\n",
    "\n",
    "# Positive skewness (right skew)\n",
    "data_pos_skew = np.random.exponential(scale=1, size=1000)\n",
    "\n",
    "# Negative skewness (left skew)\n",
    "data_neg_skew = -np.random.exponential(scale=1, size=1000)\n",
    "\n",
    "# Symmetric distribution (normal distribution)\n",
    "data_sym = np.random.normal(loc=0, scale=1, size=1000)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Positive Skewness\n",
    "sns.histplot(data_pos_skew, kde=True, ax=axes[0])\n",
    "axes[0].set_title('Positive Skewness (Right Skew)')\n",
    "\n",
    "# Negative Skewness\n",
    "sns.histplot(data_neg_skew, kde=True, ax=axes[1])\n",
    "axes[1].set_title('Negative Skewness (Left Skew)')\n",
    "\n",
    "# Symmetric Distribution\n",
    "sns.histplot(data_sym, kde=True, ax=axes[2])\n",
    "axes[2].set_title('Symmetric Distribution (Normal)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3efabb-4136-41ee-81cf-73fdfd9ea33b",
   "metadata": {},
   "source": [
    "# 22. Explain PROBABILITY MASS FUNCTION (PMF) and PROBABILITY DENSITY FUNCTION (PDF). and what is the difference between them?\n",
    "\n",
    "| **Aspect**                     | **Probability Mass Function (PMF)**                     | **Probability Density Function (PDF)**                  |\n",
    "|--------------------------------|---------------------------------------------------------|---------------------------------------------------------|\n",
    "| **Definition**                 | Used for discrete random variables.                    | Used for continuous random variables.                  |\n",
    "| **Probability Calculation**    | Gives the probability of exact values.                 | Describes density; probability is found over intervals.|\n",
    "| **Properties**                 | - Probability of specific value \\( P(X = x) \\).        | - Density at a value \\( f(x) \\).                        |\n",
    "|                                | - Sum of all PMF values equals 1.                      | - Total area under the curve equals 1.                 |\n",
    "| **Example**                    | Rolling a fair six-sided die (e.g., \\( \\frac{1}{6} \\) for each face). | Heights of adults with a bell-shaped curve.            |\n",
    "| **Type of Variable**           | Discrete random variables.                             | Continuous random variables.                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc0af7-8483-48be-aa9f-f05b8667288e",
   "metadata": {},
   "source": [
    "# 23. What is correlation. Explain its type in details.what are the methods of determining correlation\n",
    "\n",
    "| **Aspect**                      | **Description**                                                                                     |\n",
    "|---------------------------------|-----------------------------------------------------------------------------------------------------|\n",
    "| **Correlation**                 | Measures the strength and direction of a relationship between two variables.                      |\n",
    "\n",
    "| **Type**                        | **Definition**                                                                                     | **Example**                          |\n",
    "|---------------------------------|-----------------------------------------------------------------------------------------------------|--------------------------------------|\n",
    "| **Positive Correlation**        | Two variables increase or decrease together.                                                       | Height and weight.                   |\n",
    "| **Negative Correlation**        | One variable increases while the other decreases.                                                  | Study hours and test errors.         |\n",
    "| **Zero Correlation**            | No linear relationship between the variables.                                                      | Shoe size and intelligence.          |\n",
    "\n",
    "| **Method**                      | **Definition**                                                                                     | **Formula**                                                                             |\n",
    "|---------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|\n",
    "| **Pearson Correlation Coefficient** | Measures linear relationship between two continuous variables.                                     | \\( r = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} \\)                                      |\n",
    "| **Spearman's Rank Correlation** | Measures the association between two ranked variables.                                              | \\( \\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)} \\)                                          |\n",
    "| **Kendall's Tau**               | Measures association between variables based on ranks, used for smaller sample sizes.                | \\( \\tau = \\frac{(C - D)}{\\sqrt{(C + D + T_x)(C + D + T_y)}} \\)                           |\n",
    "| **Point-Biserial Correlation**  | Measures correlation between a continuous variable and a binary variable.                           | \\( r_{pb} = \\frac{M_1 - M_0}{\\sigma} \\sqrt{\\frac{p_1 p_0}{n}} \\)                        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46c18b-a412-4855-a918-0a986914cbf7",
   "metadata": {},
   "source": [
    "# 24. Calculate coefficient of correlation between the marks obtained by 10 students in Accountancy and statistics:\n",
    "##### Use Karl Pearson’s Coefficient of Correlation Method to find it.\n",
    "\n",
    "| **Student**     | 1  | 2  | 3  | 4  | 5  | 6  | 7  | 8  | 9  | 10 |\n",
    "|-----------------|----|----|----|----|----|----|----|----|----|----|\n",
    "| **Accountancy** | 45 | 70 | 65 | 30 | 90 | 40 | 50 | 75 | 85 | 60 |\n",
    "| **Statistics**  | 35 | 90 | 70 | 40 | 95 | 40 | 60 | 80 | 80 | 50 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c474a6-4a6f-48e1-8166-eb3151372686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pearson correlation coefficient r is :  0.9031178882610624\n",
      "This value indicates a strong positive correlation between the marks obtained in Accountancy and Statistics.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "accountancy = np.array([45, 70, 65, 30, 90, 40, 50, 75, 85, 60])\n",
    "statistics = np.array([35, 90, 70, 40, 95, 40, 60, 80, 80, 50])\n",
    "\n",
    "n = len(accountancy)\n",
    "\n",
    "# Calculate the sums and the sum of the products\n",
    "sum_x = np.sum(accountancy)\n",
    "sum_y = np.sum(statistics)\n",
    "sum_xy = np.sum(accountancy * statistics)\n",
    "sum_x2 = np.sum(accountancy**2)\n",
    "sum_y2 = np.sum(statistics**2)\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "numerator = n * sum_xy - sum_x * sum_y\n",
    "denominator = np.sqrt((n * sum_x2 - sum_x**2) * (n * sum_y2 - sum_y**2))\n",
    "r = numerator / denominator\n",
    "\n",
    "print(\"The Pearson correlation coefficient r is : \" , r)\n",
    "print(\"This value indicates a strong positive correlation between the marks obtained in Accountancy and Statistics.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401ad71-f253-4407-9154-edd42fd5957b",
   "metadata": {},
   "source": [
    "# 25. Discuss the 4 differences between correlation and regression.\n",
    "\n",
    "| **Aspect**        | **Correlation**                                                                                        | **Regression**                                                                                      |\n",
    "|-------------------|--------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|\n",
    "| **Definition**    | Measures the strength and direction of the linear relationship between two variables.                  | Predicts the value of a dependent variable based on the value of one or more independent variables. |\n",
    "| **Purpose**       | To quantify the degree to which two variables are related.                                             | To model the relationship between variables and make predictions.                                   |\n",
    "| **Symmetry**      | Symmetric; correlation between \\(X\\) and \\(Y\\) is the same as between \\(Y\\) and \\(X\\).                  | Asymmetric; regression of \\(Y\\) on \\(X\\) is different from regression of \\(X\\) on \\(Y\\).            |\n",
    "| **Units**         | Dimensionless; has no units.                                                                           | Dependent on the units of the variables involved.                                                   |\n",
    "| **Output**        | A single value (correlation coefficient) ranging from -1 to +1.                                         | An equation that describes the relationship between variables (e.g., \\(Y = a + bX\\)).               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2b2c7-c75f-4d33-9bd7-07b821d7c483",
   "metadata": {},
   "source": [
    "# 26. Find the most likely price in Delhi corresponding to the price of Rs. 70 in Agra given that the coefficient of correlation between the prices of the two places is +0.8.\n",
    "\n",
    "### \n",
    "we can use the linear relationship between the prices in the two locations. \n",
    "\n",
    "1. **Assumptions**:\n",
    "   - Let \\( P_D \\) be the price in Delhi.\n",
    "   - Let \\( P_A \\) be the price in Agra.\n",
    "   - Coefficient of correlation between the prices in Agra and Delhi: \\( r = 0.8 \\).\n",
    "   - Mean prices and standard deviations are not provided.\n",
    "\n",
    "2. **Formula for Prediction**:\n",
    "   \\[\n",
    "   P_D = \\bar{P_D} + \\frac{r \\cdot \\sigma_D}{\\sigma_A} \\cdot (P_A - \\bar{P_A})\n",
    "   \\]\n",
    "   where:\n",
    "   - \\( \\bar{P_A} \\) = Mean price in Agra\n",
    "   - \\( \\bar{P_D} \\) = Mean price in Delhi\n",
    "   - \\( \\sigma_A \\) = Standard deviation of prices in Agra\n",
    "   - \\( \\sigma_D \\) = Standard deviation of prices in Delhi\n",
    "   - \\( r \\) = Coefficient of correlation (0.8)\n",
    "\n",
    "3. **Approximate Calculation**:\n",
    "   Without the mean prices and standard deviations, you can approximate the price in Delhi using a proportional relationship:\n",
    "   \\[\n",
    "   P_D \\approx \\text{Mean price in Delhi} + 0.8 \\times (P_A - \\text{Mean price in Agra})\n",
    "   \\]\n",
    "\n",
    "   If specific values for means and standard deviations are provided, use the exact formula for precise calculation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4060f7-4cff-45f2-b424-b53c010ed270",
   "metadata": {},
   "source": [
    "# 27. In a partially destroyed laboratory record of an analysis of correlation data, the following results only are legible: Variance of x = 9, Regression equations are: (i) 8x−10y = −66; (ii) 40x − 18y = 214. What are (a) the mean values of x and y, (b) the coefficient of correlation between x and y, (c) the σ of y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9c9d4d-7127-4731-b584-39689e33307f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of x = 13, mean of y =  17\n",
      "coefficient of correlation between x and y = 0.6000000000000001 \n",
      "the σ of y = 4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sympy import symbols, Eq, solve\n",
    "\n",
    "variance_x = 9\n",
    "sigma_x = np.sqrt(variance_x)\n",
    "\n",
    "# Regression equations\n",
    "# 8x - 10y = -66\n",
    "# 40x - 18y = 214\n",
    "\n",
    "# Solving the equations for the mean values of x and y\n",
    "x, y = symbols('x y')\n",
    "eq1 = Eq(8*x - 10*y, -66)\n",
    "eq2 = Eq(40*x - 18*y, 214)\n",
    "solution = solve((eq1, eq2), (x, y))\n",
    "\n",
    "mean_x = solution[x]\n",
    "mean_y = solution[y]\n",
    "\n",
    "# Regression coefficients\n",
    "b_yx = 8 / 10  # Slope from first regression equation\n",
    "b_xy = 18 / 40  # Slope from second regression equation\n",
    "\n",
    "# Coefficient of correlation\n",
    "r = np.sqrt(b_yx * b_xy)\n",
    "\n",
    "# Standard deviation of y\n",
    "sigma_y = b_yx * sigma_x / r\n",
    "\n",
    "print(f\"mean of x = {mean_x}, mean of y =  {mean_y}\\ncoefficient of correlation between x and y = {r} \\nthe σ of y = {sigma_y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280b805-7707-4009-a6d6-6b0a432cf55e",
   "metadata": {},
   "source": [
    "# 28. What is Normal Distribution? What are the four Assumptions of Normal Distribution? Explain in detail.\n",
    "\n",
    "##### Normal Distribution, also known as the Gaussian distribution, is a continuous probability distribution that is symmetrical around its mean. It is characterized by its bell-shaped curve, where most of the observations cluster around the central peak and the probabilities for values further away from the mean taper off equally in both directions.\n",
    "\n",
    "###### Key Characteristics:\n",
    "* Symmetry: The left and right sides of the curve are mirror images.\n",
    "* Mean, Median, and Mode: All are located at the center of the distribution.\n",
    "* Asymptotic: The tails of the distribution approach the horizontal axis but never touch it.\n",
    "* Defined by Mean (μ) and Standard Deviation (σ): The mean determines the location of the center, and the standard deviation determines the spread of the distribution.\n",
    "\n",
    "###### Four Assumptions of Normal Distribution\n",
    "\n",
    "###### Independence:\n",
    "* Assumption: Observations are independent of each other.\n",
    "* Explanation: The value of one observation does not influence the value of another. This is crucial for many statistical methods that assume the normal distribution, as dependence between observations can lead to incorrect conclusions.\n",
    "\n",
    "###### Normality (Distribution of Errors):\n",
    "* Assumption: The data, or the errors in the data, are normally distributed.\n",
    "* Explanation: For many statistical tests, especially parametric tests, the residuals (differences between observed and predicted values) are assumed to be normally distributed. This ensures that the statistical tests have valid results.\n",
    "\n",
    "###### Homoscedasticity (Constant Variance):\n",
    "* Assumption: The variance among the observations is constant.\n",
    "* Explanation: Homoscedasticity means that the spread or variability in the data is the same across all levels of an independent variable. When this assumption is violated, it can lead to biased or inefficient estimates.\n",
    "\n",
    "###### Linearity:\n",
    "* Assumption: The relationship between the independent and dependent variables is linear.\n",
    "* Explanation: For many analyses, it is assumed that there is a straight-line relationship between the variables. If the relationship is not linear, the results of the analysis may not be accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05683e-3489-4017-920d-467c70588b55",
   "metadata": {},
   "source": [
    "# 29.Write all the characteristics or Properties of the Normal Distribution Curve.\n",
    "\n",
    "\n",
    "| **Characteristic**                        | **Description**                                                                                                                                           |\n",
    "|-------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Symmetry**                              | The normal distribution curve is symmetric about the mean, meaning the left side is a mirror image of the right side.                                      |\n",
    "| **Bell-Shaped Curve**                     | The curve is bell-shaped, with a single peak at the mean (\\( \\mu \\)).                                                                                      |\n",
    "| **Mean, Median, and Mode**                | For a normal distribution, the mean (\\( \\mu \\)), median, and mode are all equal and located at the center of the distribution.                             |\n",
    "| **Asymptotic**                            | The tails of the normal distribution curve approach the horizontal axis but never touch or cross it, extending infinitely in both directions.              |\n",
    "| **Empirical Rule (68-95-99.7 Rule)**      | - Approximately 68% of the data falls within one standard deviation (\\( \\sigma \\)) of the mean.                                                            |\n",
    "|                                           | - Approximately 95% of the data falls within two standard deviations (\\( \\sigma \\)) of the mean.                                                           |\n",
    "|                                           | - Approximately 99.7% of the data falls within three standard deviations (\\( \\sigma \\)) of the mean.                                                       |\n",
    "| **Defined by Mean and Standard Deviation**| The shape and position of the normal distribution are determined by the mean (\\( \\mu \\)) and the standard deviation (\\( \\sigma \\)).                        |\n",
    "| **Total Area Under the Curve**            | The total area under the normal distribution curve is equal to 1, representing the total probability of all possible outcomes.                             |\n",
    "| **Continuous Distribution**               | The normal distribution is a continuous probability distribution, describing the probabilities of a continuous random variable.                            |\n",
    "| **Unimodal**                              | The normal distribution has a single peak (mode) at the mean, making it unimodal.                                                                          |\n",
    "| **No Skewness**                           | The normal distribution curve has a skewness of 0, indicating perfect symmetry.                                                                             |\n",
    "| **Kurtosis**                              | The normal distribution has a kurtosis of 0, indicating a mesokurtic shape (neither too peaked nor too flat).                                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62aa3a-c04c-411c-8b06-0798e89baa40",
   "metadata": {},
   "source": [
    "# 30.Which of the following options are correct about Normal Distribution Curve.\n",
    "* (a) Within a range 0.6745 of σ on both sides the middle 50% of the observations occur i,e. mean ±0.6745σ covers 50% area 25% on each side.\n",
    "* (b) Mean ±1S.D. (i,e.μ ± 1σ) covers 68.268% area, 34.134 % area lies on either side of the mean.\n",
    "* (c) Mean ±2S.D. (i,e. μ ± 2σ) covers 95.45% area, 47.725% area lies on either side of the mean.\n",
    "* (d) Mean ±3 S.D. (i,e. μ ±3σ) covers 99.73% area, 49.856% area lies on the either side of the mean.\n",
    "* (e) Only 0.27% area is outside the range μ ±3σ.\n",
    "\n",
    "##### a) Correct: Mean ±0.6745σ covers the middle 50% of observations.\n",
    "##### (b) Correct: Mean ±1σ covers 68.268% of the area, with 34.134% on each side.\n",
    "##### (c) Correct: Mean ±2σ covers 95.45% of the area, with 47.725% on each side.\n",
    "##### (d) Incorrect: Mean ±3σ covers 99.73% of the area; 49.865% on each side, not 49.856%.\n",
    "##### (e) Correct: 0.27% of the area is outside Mean ±3σ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c60537-a4bb-439a-bf6e-a4b5cae815c9",
   "metadata": {},
   "source": [
    "# 31. The mean of a distribution is 60 with a standard deviation of 10. Assuming that the distribution is normal, what percentage of items be (i) between 60 and 72, (ii) between 50 and 60, (iii) beyond 72 and (iv) between 70 and 80?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0bb5e5c-6c3b-47a1-8e18-1fbdf500b55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of items between 60 and 72: 38.49%\n",
      "Percentage of items between 50 and 60: 34.13%\n",
      "Percentage of items beyond 72: 11.51%\n",
      "Percentage of items between 70 and 80: 13.59%\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "mean = 60\n",
    "std_dev = 10\n",
    "\n",
    "# (i) Percentage of items between 60 and 72\n",
    "z1 = (60 - mean) / std_dev\n",
    "z2 = (72 - mean) / std_dev\n",
    "percent_between_60_and_72 = (stats.norm.cdf(z2) - stats.norm.cdf(z1)) * 100\n",
    "\n",
    "# (ii) Percentage of items between 50 and 60\n",
    "z1 = (50 - mean) / std_dev\n",
    "z2 = (60 - mean) / std_dev\n",
    "percent_between_50_and_60 = (stats.norm.cdf(z2) - stats.norm.cdf(z1)) * 100\n",
    "\n",
    "# (iii) Percentage of items beyond 72\n",
    "z = (72 - mean) / std_dev\n",
    "percent_beyond_72 = (1 - stats.norm.cdf(z)) * 100\n",
    "\n",
    "# (iv) Percentage of items between 70 and 80\n",
    "z1 = (70 - mean) / std_dev\n",
    "z2 = (80 - mean) / std_dev\n",
    "percent_between_70_and_80 = (stats.norm.cdf(z2) - stats.norm.cdf(z1)) * 100\n",
    "\n",
    "print(f\"Percentage of items between 60 and 72: {percent_between_60_and_72:.2f}%\")\n",
    "print(f\"Percentage of items between 50 and 60: {percent_between_50_and_60:.2f}%\")\n",
    "print(f\"Percentage of items beyond 72: {percent_beyond_72:.2f}%\")\n",
    "print(f\"Percentage of items between 70 and 80: {percent_between_70_and_80:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7d406-e005-4472-a1b2-e9d5e8d53f6b",
   "metadata": {},
   "source": [
    "# 32. 15000 students sat for an examination. The mean marks was 49 and the distribution of marks had a standard deviation of 6. Assuming that the marks were normally distributed what proportion of students scored (a) more than 55 marks, (b) more than 70 marks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed6e49f-b52d-4c73-98d5-bc0fa7aef491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of students scoring more than 55 marks: 0.1587 (15.87%)\n",
      "Proportion of students scoring more than 70 marks: 0.0002 (0.02%)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "mean = 49\n",
    "std_dev = 6\n",
    "\n",
    "# (a) Proportion of students scoring more than 55 marks\n",
    "z1 = (55 - mean) / std_dev\n",
    "proportion_more_than_55 = 1 - stats.norm.cdf(z1)\n",
    "\n",
    "# (b) Proportion of students scoring more than 70 marks\n",
    "z2 = (70 - mean) / std_dev\n",
    "proportion_more_than_70 = 1 - stats.norm.cdf(z2)\n",
    "\n",
    "print(f\"Proportion of students scoring more than 55 marks: {proportion_more_than_55:.4f} ({proportion_more_than_55*100:.2f}%)\")\n",
    "print(f\"Proportion of students scoring more than 70 marks: {proportion_more_than_70:.4f} ({proportion_more_than_70*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cc933d-5440-4624-a9be-a933edc6a633",
   "metadata": {},
   "source": [
    "# 33. If the height of 500 students are normally distributed with mean 65 inch and standard deviation 5 inch. How many students have height : a) greater than 70 inch. b) between 60 and 70 inch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47b7712f-945b-4975-80d1-5ec650f95b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of students with height greater than 70 inches: 79\n",
      "Number of students with height between 60 and 70 inches: 341\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "mean = 65\n",
    "std_dev = 5\n",
    "total_students = 500\n",
    "\n",
    "# (a) Number of students with height greater than 70 inches\n",
    "z1 = (70 - mean) / std_dev\n",
    "proportion_more_than_70 = 1 - stats.norm.cdf(z1)\n",
    "number_more_than_70 = proportion_more_than_70 * total_students\n",
    "\n",
    "# (b) Number of students with height between 60 and 70 inches\n",
    "z2 = (60 - mean) / std_dev\n",
    "z3 = (70 - mean) / std_dev\n",
    "proportion_between_60_and_70 = stats.norm.cdf(z3) - stats.norm.cdf(z2)\n",
    "number_between_60_and_70 = proportion_between_60_and_70 * total_students\n",
    "\n",
    "print(f\"Number of students with height greater than 70 inches: {round(number_more_than_70)}\")\n",
    "print(f\"Number of students with height between 60 and 70 inches: {round(number_between_60_and_70)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c01265c-6ecf-4c7e-9d03-3c67c3714d12",
   "metadata": {},
   "source": [
    "# 34. What is the statistical hypothesis? Explain the errors in hypothesis testing.b)Explain the Sample. What are Large Samples & Small Samples?\n",
    "\n",
    "| **Concept**                 | **Description**                                                                                           |\n",
    "|-----------------------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| **Statistical Hypothesis**  | Statement or assumption about a population parameter that can be tested using statistical methods.       |\n",
    "| **Null Hypothesis (\\( H_0 \\))**   | Assumes no effect or difference (default assumption).                                                   |\n",
    "| **Alternative Hypothesis (\\( H_A \\))** | Assumes an effect or difference exists (what you aim to prove).                                        |\n",
    "| **Type I Error**            | Incorrectly rejecting the null hypothesis when it is true (False Positive).                                |\n",
    "| **Type II Error**           | Incorrectly accepting the null hypothesis when the alternative is true (False Negative).                   |\n",
    "| **Sample**                  | Subset of individuals or items selected from a population to make inferences about the population.          |\n",
    "| **Large Sample**            | Sample size greater than 30; sampling distribution of the mean approximates normality.                    |\n",
    "| **Small Sample**            | Sample size of 30 or less; may not approximate normality, requiring different statistical methods.         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c06a34d-1d81-486d-8214-5a1ffff1234e",
   "metadata": {},
   "source": [
    "# 35.A random sample of size 25 from a population gives the sample standard derivation to be 9.0. Test the hypothesis that the population standard derivation is 10.5. Hint(Use chi-square distribution).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e296d78-26c9-4bf4-bf9a-a2a7006c6e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square Test Statistic: 17.63\n",
      "Critical Value Lower: 12.40\n",
      "Critical Value Upper: 39.36\n",
      "Do not reject the null hypothesis.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "n = 25\n",
    "s = 9.0\n",
    "sigma_0 = 10.5\n",
    "alpha = 0.05\n",
    "\n",
    "# Calculate sample variance\n",
    "s2 = s**2\n",
    "\n",
    "# Calculate the chi-square test statistic\n",
    "chi_square_statistic = (n - 1) * s2 / sigma_0**2\n",
    "\n",
    "# Degrees of freedom\n",
    "df = n - 1\n",
    "\n",
    "# Critical values for a two-tailed test\n",
    "critical_value_lower = stats.chi2.ppf(alpha / 2, df)\n",
    "critical_value_upper = stats.chi2.ppf(1 - alpha / 2, df)\n",
    "\n",
    "print(f\"Chi-square Test Statistic: {chi_square_statistic:.2f}\")\n",
    "print(f\"Critical Value Lower: {critical_value_lower:.2f}\")\n",
    "print(f\"Critical Value Upper: {critical_value_upper:.2f}\")\n",
    "\n",
    "if chi_square_statistic < critical_value_lower or chi_square_statistic > critical_value_upper:\n",
    "    print(\"Reject the null hypothesis.\")\n",
    "else:\n",
    "    print(\"Do not reject the null hypothesis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17db8a-e2ac-4577-93af-a528bf3c46bb",
   "metadata": {},
   "source": [
    "# 37.100 students of a PW IOI obtained the following grades in Data Science paper:\n",
    "* Grade :[A, B, C, D, E]\n",
    "* Total Frequency :[15, 17, 30, 22, 16, 100]\n",
    "###### Using the χ 2 test , examine the hypothesis that the distribution of grades is uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aff3a62-3778-4d6e-befa-261beccfef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square Test Statistic: 7.70\n",
      "Critical Value: 9.49\n",
      "Do not reject the null hypothesis.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "observed_frequencies = [15, 17, 30, 22, 16]\n",
    "expected_frequency = 20\n",
    "total_grades = len(observed_frequencies)\n",
    "df = total_grades - 1\n",
    "alpha = 0.05\n",
    "\n",
    "# Calculate chi-square test statistic\n",
    "chi_square_statistic = sum((obs - expected_frequency) ** 2 / expected_frequency for obs in observed_frequencies)\n",
    "\n",
    "# Critical value from chi-square distribution\n",
    "critical_value = stats.chi2.ppf(1 - alpha, df)\n",
    "\n",
    "print(f\"Chi-square Test Statistic: {chi_square_statistic:.2f}\")\n",
    "print(f\"Critical Value: {critical_value:.2f}\")\n",
    "\n",
    "if chi_square_statistic > critical_value:\n",
    "    print(\"Reject the null hypothesis.\")\n",
    "else:\n",
    "    print(\"Do not reject the null hypothesis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6689cabb-c520-4610-ba05-1859132945ac",
   "metadata": {},
   "source": [
    "# 38.Anova Test: To study the performance of three detergents and three different water temperatures the following whiteness readings were obtained with specially designed equipment.\n",
    "\n",
    "| **Water Temp** | **Detergents A** | **Detergents B** | **Detergents C** |\n",
    "|----------------|------------------|------------------|------------------|\n",
    "| Cold Water     | 57               | 55               | 67               |\n",
    "| Warm Water     | 49               | 52               | 68               |\n",
    "| Hot Water      | 54               | 46               | 58               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568d097f-9a88-4ed4-9f61-4775069ae545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>df</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C(Water_Temp)</th>\n",
       "      <td>73.555556</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.381295</td>\n",
       "      <td>0.208380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C(Detergent)</th>\n",
       "      <td>304.222222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.848921</td>\n",
       "      <td>0.028491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>61.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sum_sq   df         F    PR(>F)\n",
       "C(Water_Temp)   73.555556  2.0  2.381295  0.208380\n",
       "C(Detergent)   304.222222  2.0  9.848921  0.028491\n",
       "Residual        61.777778  4.0       NaN       NaN"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Water_Temp': ['Cold Water', 'Cold Water', 'Cold Water', 'Warm Water', 'Warm Water', 'Warm Water', 'Hot Water', 'Hot Water', 'Hot Water'],\n",
    "    'Detergent': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],\n",
    "    'Whiteness': [57, 55, 67, 49, 52, 68, 54, 46, 58]\n",
    "})\n",
    "\n",
    "# Perform two-way ANOVA without interaction term\n",
    "model_simple = ols('Whiteness ~ C(Water_Temp) + C(Detergent)', data=data).fit()\n",
    "anova_table_simple = sm.stats.anova_lm(model_simple, typ=2)\n",
    "\n",
    "anova_table_simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47dd932-0263-49c4-951d-c2b20a73fb1b",
   "metadata": {},
   "source": [
    "#### Interpretation:\n",
    "* Water_Temp: The p-value (0.208) is greater than 0.05, suggesting that the different water temperatures do not have a statistically significant effect on the whiteness readings.\n",
    "* Detergent: The p-value (0.028) is less than 0.05, indicating that the different detergents have a statistically significant effect on the whiteness readings.\n",
    "#### Conclusion:\n",
    "* The type of detergent used significantly affects the whiteness readings, whereas the water temperature does not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99acd94d-0e60-4ca3-ad38-aa0dc8b88c22",
   "metadata": {},
   "source": [
    "# 39.How would you create a basic Flask route that displays \"Hello, World!\" on the homepage?\n",
    "\n",
    "1. **Install Flask** (if you haven't already):\n",
    "\n",
    "    ```bash\n",
    "    pip install Flask\n",
    "    ```\n",
    "\n",
    "2. **Create a new Python file** (e.g., `app.py`) and add the following code:\n",
    "\n",
    "    ```python\n",
    "    from flask import Flask\n",
    "\n",
    "    # Create a Flask application instance\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    # Define a route for the homepage\n",
    "    @app.route('/')\n",
    "    def hello_world():\n",
    "        return 'Hello, World!'\n",
    "\n",
    "    # Run the application\n",
    "    if __name__ == '__main__':\n",
    "        app.run(debug=True)\n",
    "    ```\n",
    "\n",
    "3. **Run the application**:\n",
    "\n",
    "    In terminal, navigate to the directory containing `app.py` and run:\n",
    "\n",
    "    ```bash\n",
    "    python app.py\n",
    "    ```\n",
    "\n",
    "4. **Open the web browser** and go to `http://127.0.0.1:5000/`. there we can see \"Hello, World!\" displayed on the homepage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b764f8-b529-49f7-a54e-b26d7c190af0",
   "metadata": {},
   "source": [
    "# 40.Explain how to set up a Flask application to handle form submissions using POST requests.\n",
    "\n",
    "1. **Install Flask** (if you haven't already):\n",
    "\n",
    "    ```bash\n",
    "    pip install Flask\n",
    "    ```\n",
    "\n",
    "2. **Create a Flask Application**\n",
    "\n",
    "    Create a Python file (e.g., `app.py`) with the following code:\n",
    "\n",
    "    ```python\n",
    "    from flask import Flask, request\n",
    "\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    # Route to display the form\n",
    "    @app.route('/', methods=['GET'])\n",
    "    def form():\n",
    "        return '''\n",
    "            <form method=\"POST\" action=\"/submit\">\n",
    "                <label for=\"name\">Name:</label>\n",
    "                <input type=\"text\" id=\"name\" name=\"name\" required>\n",
    "                <br>\n",
    "                <label for=\"email\">Email:</label>\n",
    "                <input type=\"email\" id=\"email\" name=\"email\" required>\n",
    "                <br>\n",
    "                <input type=\"submit\" value=\"Submit\">\n",
    "            </form>\n",
    "        '''\n",
    "\n",
    "    # Route to handle form submission\n",
    "    @app.route('/submit', methods=['POST'])\n",
    "    def submit():\n",
    "        name = request.form.get('name')\n",
    "        email = request.form.get('email')\n",
    "        # Process the form data (e.g., save it, send an email, etc.)\n",
    "        return f\"Form submitted! Name: {name}, Email: {email}\"\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        app.run(debug=True)\n",
    "    ```\n",
    "\n",
    "3. **Run the Application**\n",
    "\n",
    "    In terminal, navigate to the directory containing `app.py` and run:\n",
    "\n",
    "    ```bash\n",
    "    python app.py\n",
    "    ```\n",
    "\n",
    "4. **Test the Form**\n",
    "\n",
    "    - Open the web browser and go to `http://127.0.0.1:5000/`.\n",
    "    - can see a form with fields for \"Name\" and \"Email\".\n",
    "    - Fill out the form and submit it.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **Form Display Route (`/`):** This route uses the `GET` method to display a simple HTML form. The form uses the `POST` method to submit data to the `/submit` endpoint.\n",
    "- **Form Submission Route (`/submit`):** This route handles the form submission. It uses the `POST` method to retrieve form data using `request.form.get('field_name')`. heere we can process the data as needed (e.g., save to a database, send an email).\n",
    "- **Redirect or Response:** After processing the form, we can redirect the user to another page or display a response. In this example, we simply return a message with the submitted data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da409dbc-c45f-424f-a86b-6f7ae862592c",
   "metadata": {},
   "source": [
    "# 41.Write a Flask route that accepts a parameter in the URL and displays it on the page.\n",
    "\n",
    "1. **Create a Flask Application**\n",
    "\n",
    "    Create a Python file (e.g., `app.py`) with the following code:\n",
    "\n",
    "    ```python\n",
    "    from flask import Flask\n",
    "\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    # Route that accepts a parameter in the URL\n",
    "    @app.route('/greet/<name>')\n",
    "    def greet(name):\n",
    "        return f'Hello, {name}!'\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        app.run(debug=True)\n",
    "    ```\n",
    "\n",
    "2. **Run the Application**\n",
    "\n",
    "    In terminal, navigate to the directory containing `app.py` and run:\n",
    "\n",
    "    ```bash\n",
    "    python app.py\n",
    "    ```\n",
    "\n",
    "3. **Test the Route**\n",
    "\n",
    "    - Open the web browser and go to `http://127.0.0.1:5000/greet/John`.\n",
    "    - can see \"Hello, John!\" displayed on the page.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **Route Definition (`/greet/<name>`):** The `<name>` in the route URL is a placeholder for a variable part of the URL. Flask will pass the value of this part of the URL to the `greet` function as the `name` parameter.\n",
    "- **Route Handler (`greet(name)`):** The `greet` function receives the `name` parameter from the URL and returns a string that includes this value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec229f5-aa34-421d-95eb-c9b144de6db8",
   "metadata": {},
   "source": [
    "# 42.How can you implement user authentication in a Flask application?\n",
    "\n",
    "1. **Install Required Packages**\n",
    "\n",
    "    First, install Flask, Flask-Login, and Flask-SQLAlchemy:\n",
    "\n",
    "    ```bash\n",
    "    pip install Flask Flask-Login Flask-SQLAlchemy\n",
    "    ```\n",
    "\n",
    "2. **Create a Flask Application**\n",
    "\n",
    "    Create a Python file (e.g., `app.py`) with the following code:\n",
    "\n",
    "    ```python\n",
    "    from flask import Flask, render_template, redirect, url_for, request\n",
    "    from flask_sqlalchemy import SQLAlchemy\n",
    "    from flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\n",
    "\n",
    "    app = Flask(__name__)\n",
    "    app.config['SECRET_KEY'] = 'your_secret_key'\n",
    "    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///users.db'\n",
    "    db = SQLAlchemy(app)\n",
    "    login_manager = LoginManager(app)\n",
    "    login_manager.login_view = 'login'\n",
    "\n",
    "    # Define the User model\n",
    "    class User(db.Model, UserMixin):\n",
    "        id = db.Column(db.Integer, primary_key=True)\n",
    "        username = db.Column(db.String(150), unique=True, nullable=False)\n",
    "        password = db.Column(db.String(150), nullable=False)\n",
    "\n",
    "    # Load user function for Flask-Login\n",
    "    @login_manager.user_loader\n",
    "    def load_user(user_id):\n",
    "        return User.query.get(int(user_id))\n",
    "\n",
    "    @app.route('/login', methods=['GET', 'POST'])\n",
    "    def login():\n",
    "        if request.method == 'POST':\n",
    "            username = request.form['username']\n",
    "            password = request.form['password']\n",
    "            user = User.query.filter_by(username=username).first()\n",
    "            if user and user.password == password:  # Simplified for demonstration; use hashed passwords in production\n",
    "                login_user(user)\n",
    "                return redirect(url_for('profile'))\n",
    "        return render_template('login.html')\n",
    "\n",
    "    @app.route('/register', methods=['GET', 'POST'])\n",
    "    def register():\n",
    "        if request.method == 'POST':\n",
    "            username = request.form['username']\n",
    "            password = request.form['password']\n",
    "            new_user = User(username=username, password=password)\n",
    "            db.session.add(new_user)\n",
    "            db.session.commit()\n",
    "            return redirect(url_for('login'))\n",
    "        return render_template('register.html')\n",
    "\n",
    "    @app.route('/profile')\n",
    "    @login_required\n",
    "    def profile():\n",
    "        return f'Hello, {current_user.username}! Welcome to your profile.'\n",
    "\n",
    "    @app.route('/logout')\n",
    "    @login_required\n",
    "    def logout():\n",
    "        logout_user()\n",
    "        return redirect(url_for('login'))\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        db.create_all()  # Create database tables\n",
    "        app.run(debug=True)\n",
    "    ```\n",
    "\n",
    "3. **Create HTML Templates**\n",
    "\n",
    "    Create a `templates` directory with the following files:\n",
    "\n",
    "    **`templates/login.html`:**\n",
    "\n",
    "    ```html\n",
    "    <!doctype html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <title>Login</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Login</h1>\n",
    "        <form method=\"POST\">\n",
    "            <label for=\"username\">Username:</label>\n",
    "            <input type=\"text\" id=\"username\" name=\"username\" required>\n",
    "            <br>\n",
    "            <label for=\"password\">Password:</label>\n",
    "            <input type=\"password\" id=\"password\" name=\"password\" required>\n",
    "            <br>\n",
    "            <input type=\"submit\" value=\"Login\">\n",
    "        </form>\n",
    "        <a href=\"{{ url_for('register') }}\">Register</a>\n",
    "    </body>\n",
    "    </html>\n",
    "    ```\n",
    "\n",
    "    **`templates/register.html`:**\n",
    "\n",
    "    ```html\n",
    "    <!doctype html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <title>Register</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Register</h1>\n",
    "        <form method=\"POST\">\n",
    "            <label for=\"username\">Username:</label>\n",
    "            <input type=\"text\" id=\"username\" name=\"username\" required>\n",
    "            <br>\n",
    "            <label for=\"password\">Password:</label>\n",
    "            <input type=\"password\" id=\"password\" name=\"password\" required>\n",
    "            <br>\n",
    "            <input type=\"submit\" value=\"Register\">\n",
    "        </form>\n",
    "        <a href=\"{{ url_for('login') }}\">Login</a>\n",
    "    </body>\n",
    "    </html>\n",
    "    ```\n",
    "\n",
    "4. **Run the Application**\n",
    "\n",
    "    Navigate to the directory containing `app.py` and run:\n",
    "\n",
    "    ```bash\n",
    "    python app.py\n",
    "    ```\n",
    "\n",
    "5. **Test the Application**\n",
    "\n",
    "    - **Register**: Go to `http://127.0.0.1:5000/register` to create a new user.\n",
    "    - **Login**: Go to `http://127.0.0.1:5000/login` to log in.\n",
    "    - **Profile**: After logging in, will be redirected to `http://127.0.0.1:5000/profile` where we can see your profile.\n",
    "    - **Logout**: Go to `http://127.0.0.1:5000/logout` to log out.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **Flask-Login:** Manages user sessions and provides `login_required` and `current_user` utilities.\n",
    "- **User Model:** Represents users in the database and uses `UserMixin` to integrate with Flask-Login.\n",
    "- **Login and Registration Routes:** Handle user authentication and registration.\n",
    "- **Profile Route:** A protected route that requires authentication.\n",
    "- **Templates:** HTML forms for login and registration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d269a3-a396-4280-8043-752e5e269a32",
   "metadata": {},
   "source": [
    "# 43.Describe the process of connecting a Flask app to a SQLite database using SQLAlchemy.\n",
    "\n",
    "1. **Install Flask and Flask-SQLAlchemy**\n",
    "\n",
    "    First, install Flask and Flask-SQLAlchemy:\n",
    "\n",
    "    ```bash\n",
    "    pip install Flask Flask-SQLAlchemy\n",
    "    ```\n",
    "\n",
    "2. **Set Up Flask Application**\n",
    "\n",
    "    Create a Python file (e.g., `app.py`) and set up your Flask application with SQLAlchemy:\n",
    "\n",
    "    ```python\n",
    "    from flask import Flask\n",
    "    from flask_sqlalchemy import SQLAlchemy\n",
    "\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    # Configure the database URI\n",
    "    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///example.db'\n",
    "    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n",
    "\n",
    "    # Initialize SQLAlchemy\n",
    "    db = SQLAlchemy(app)\n",
    "\n",
    "    # Define a model\n",
    "    class User(db.Model):\n",
    "        id = db.Column(db.Integer, primary_key=True)\n",
    "        username = db.Column(db.String(150), unique=True, nullable=False)\n",
    "        email = db.Column(db.String(150), unique=True, nullable=False)\n",
    "\n",
    "    # Create the database and tables\n",
    "    @app.before_first_request\n",
    "    def create_tables():\n",
    "        db.create_all()\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        app.run(debug=True)\n",
    "    ```\n",
    "\n",
    "3. **Define Models**\n",
    "\n",
    "    Define your database models using SQLAlchemy. In the example above, the `User` model represents a table with `id`, `username`, and `email` columns. Models should be subclasses of `db.Model`, and each attribute represents a column in the table.\n",
    "\n",
    "4. **Configure Database URI**\n",
    "\n",
    "    In the Flask app configuration, set the `SQLALCHEMY_DATABASE_URI` to the path of your SQLite database file. For SQLite, the URI format is:\n",
    "\n",
    "    ```\n",
    "    sqlite:///path/to/database.db\n",
    "    ```\n",
    "\n",
    "    - `sqlite:///example.db` means the database file `example.db` will be created in the same directory as `app.py`.\n",
    "\n",
    "5. **Initialize SQLAlchemy**\n",
    "\n",
    "    Initialize SQLAlchemy by creating an instance of `SQLAlchemy` and passing the Flask app instance to it:\n",
    "\n",
    "    ```python\n",
    "    db = SQLAlchemy(app)\n",
    "    ```\n",
    "\n",
    "6. **Create Database and Tables**\n",
    "\n",
    "    Use `db.create_all()` to create the database and tables defined by your models. This can be done manually by calling it in a route or automatically before the first request:\n",
    "\n",
    "    ```python\n",
    "    @app.before_first_request\n",
    "    def create_tables():\n",
    "        db.create_all()\n",
    "    ```\n",
    "\n",
    "7. **Use the Database**\n",
    "\n",
    "    You can now use SQLAlchemy to interact with the database. Here’s an example of adding a new user and querying the database:\n",
    "\n",
    "    ```python\n",
    "    # Adding a new user\n",
    "    new_user = User(username='john_doe', email='john@example.com')\n",
    "    db.session.add(new_user)\n",
    "    db.session.commit()\n",
    "\n",
    "    # Querying users\n",
    "    users = User.query.all()\n",
    "    for user in users:\n",
    "        print(user.username, user.email)\n",
    "    ```\n",
    "\n",
    "8. **Run the Application**\n",
    "\n",
    "    Run your Flask application:\n",
    "\n",
    "    ```bash\n",
    "    python app.py\n",
    "    ```\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Install Dependencies:** Flask and Flask-SQLAlchemy.\n",
    "2. **Configure the Database:** Set `SQLALCHEMY_DATABASE_URI` in your Flask config.\n",
    "3. **Initialize SQLAlchemy:** Create an instance of `SQLAlchemy`.\n",
    "4. **Define Models:** Use `db.Model` to define your database tables.\n",
    "5. **Create Tables:** Use `db.create_all()` to create the tables based on your models.\n",
    "6. **Interact with the Database:** Use SQLAlchemy’s ORM methods to perform database operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37decad-eb9b-42c5-a1f7-50cd3a366e2c",
   "metadata": {},
   "source": [
    "# 44.How would you create a RESTful API endpoint in Flask that returns JSON data?\n",
    "\n",
    "1. **Set Up Your Flask Application**\n",
    "\n",
    "    Create a Python file (e.g., `app.py`) with the following code:\n",
    "\n",
    "    ```python\n",
    "    from flask import Flask, jsonify\n",
    "\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    @app.route('/api/data', methods=['GET'])\n",
    "    def get_data():\n",
    "        # Sample data to be returned as JSON\n",
    "        data = {\n",
    "            'name': 'Daizy',\n",
    "            'age': 30,\n",
    "            'email': 'dg@gmail.com'\n",
    "        }\n",
    "        return jsonify(data)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        app.run(debug=True)\n",
    "    ```\n",
    "\n",
    "2. **Define a Route**\n",
    "\n",
    "    The route `/api/data` handles `GET` requests and is decorated with `@app.route`.\n",
    "\n",
    "3. **Return JSON Data**\n",
    "\n",
    "    - **Create Data:** Define a Python dictionary with the data you want to return.\n",
    "    - **Use `jsonify`:** Convert the dictionary to JSON format using `jsonify`.\n",
    "\n",
    "4. **Run Your Application**\n",
    "\n",
    "    Run the Flask application:\n",
    "\n",
    "    ```bash\n",
    "    python app.py\n",
    "    ```\n",
    "\n",
    "5. **Test the Endpoint**\n",
    "\n",
    "    - Open your browser or use a tool like `curl` or Postman.\n",
    "    - Go to `http://127.0.0.1:5000/api/data`.\n",
    "\n",
    "    You should see:\n",
    "\n",
    "    ```json\n",
    "    {\n",
    "        \"name\": \"Daizy\",\n",
    "        \"age\": 30,\n",
    "        \"email\": \"dg@gmail.com\"\n",
    "    }\n",
    "    ```\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Set Up Flask:** Create a Flask application.\n",
    "2. **Define Route:** Use `@app.route` to create the endpoint.\n",
    "3. **Return JSON Data:** Use `jsonify` to format the response as JSON.\n",
    "4. **Run and Test:** Run the app and test the endpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9cc92-9c52-4837-90ad-174458bdcdd5",
   "metadata": {},
   "source": [
    "# 45.Explain how to use Flask-WTF to create and validate forms in a Flask application.\n",
    "\n",
    "### Using Flask-WTF to Create and Validate Forms in a Flask Application\n",
    "\n",
    "To use Flask-WTF to create and validate forms, follow these steps:\n",
    "\n",
    "1. **Install Flask-WTF**\n",
    "\n",
    "    Install Flask-WTF with:\n",
    "\n",
    "    ```bash\n",
    "    pip install Flask-WTF\n",
    "    ```\n",
    "\n",
    "2. **Set Up Flask Application**\n",
    "\n",
    "    Create a Python file (e.g., `app.py`) with the following code:\n",
    "\n",
    "    ```python\n",
    "    from flask import Flask, render_template, redirect, url_for\n",
    "    from flask_wtf import FlaskForm\n",
    "    from wtforms import StringField, PasswordField, SubmitField\n",
    "    from wtforms.validators import DataRequired, Email, EqualTo\n",
    "\n",
    "    app = Flask(__name__)\n",
    "    app.config['SECRET_KEY'] = 'your_secret_key'  # Required for CSRF protection\n",
    "\n",
    "    # Define the form class\n",
    "    class LoginForm(FlaskForm):\n",
    "        username = StringField('Username', validators=[DataRequired()])\n",
    "        password = PasswordField('Password', validators=[DataRequired()])\n",
    "        submit = SubmitField('Login')\n",
    "\n",
    "    @app.route('/login', methods=['GET', 'POST'])\n",
    "    def login():\n",
    "        form = LoginForm()\n",
    "        if form.validate_on_submit():\n",
    "            # Process the form data\n",
    "            username = form.username.data\n",
    "            password = form.password.data\n",
    "            # Here you would typically verify the username and password\n",
    "            return redirect(url_for('success'))\n",
    "        return render_template('login.html', form=form)\n",
    "\n",
    "    @app.route('/success')\n",
    "    def success():\n",
    "        return \"Login successful!\"\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        app.run(debug=True)\n",
    "    ```\n",
    "\n",
    "3. **Create the Form Class**\n",
    "\n",
    "    Define a form class by subclassing `FlaskForm` and add fields and validators:\n",
    "\n",
    "    ```python\n",
    "    class LoginForm(FlaskForm):\n",
    "        username = StringField('Username', validators=[DataRequired()])\n",
    "        password = PasswordField('Password', validators=[DataRequired()])\n",
    "        submit = SubmitField('Login')\n",
    "    ```\n",
    "\n",
    "4. **Handle Forms in Routes**\n",
    "\n",
    "    Render the form in the route and handle validation:\n",
    "\n",
    "    ```python\n",
    "    @app.route('/login', methods=['GET', 'POST'])\n",
    "    def login():\n",
    "        form = LoginForm()\n",
    "        if form.validate_on_submit():\n",
    "            username = form.username.data\n",
    "            password = form.password.data\n",
    "            return redirect(url_for('success'))\n",
    "        return render_template('login.html', form=form)\n",
    "    ```\n",
    "\n",
    "5. **Create Templates**\n",
    "\n",
    "    Create `templates/login.html` to render the form:\n",
    "\n",
    "    ```html\n",
    "    <!doctype html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <title>Login</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Login</h1>\n",
    "        <form method=\"POST\" action=\"{{ url_for('login') }}\">\n",
    "            {{ form.hidden_tag() }}\n",
    "            <div>\n",
    "                {{ form.username.label }}<br>\n",
    "                {{ form.username(size=32) }}<br>\n",
    "                {% for error in form.username.errors %}\n",
    "                    <span style=\"color: red;\">[{{ error }}]</span><br>\n",
    "                {% endfor %}\n",
    "            </div>\n",
    "            <div>\n",
    "                {{ form.password.label }}<br>\n",
    "                {{ form.password(size=32) }}<br>\n",
    "                {% for error in form.password.errors %}\n",
    "                    <span style=\"color: red;\">[{{ error }}]</span><br>\n",
    "                {% endfor %}\n",
    "            </div>\n",
    "            <div>\n",
    "                {{ form.submit() }}\n",
    "            </div>\n",
    "        </form>\n",
    "    </body>\n",
    "    </html>\n",
    "    ```\n",
    "\n",
    "6. **Run Your Application**\n",
    "\n",
    "    Run your Flask application with:\n",
    "\n",
    "    ```bash\n",
    "    python app.py\n",
    "    ```\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Install Flask-WTF:** `pip install Flask-WTF`.\n",
    "2. **Set Up Flask:** Configure the app and define forms.\n",
    "3. **Create Form Class:** Use `FlaskForm` and add fields and validators.\n",
    "4. **Handle Forms:** Render and validate forms in routes.\n",
    "5. **Create Templates:** Use Jinja2 to render and handle form data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea89a1-583e-4d6c-9917-b0891b869a9b",
   "metadata": {},
   "source": [
    "# 46.How can you implement file uploads in a Flask application?\n",
    "\n",
    "### Implementing File Uploads in a Flask Application\n",
    "\n",
    "To handle file uploads in a Flask application, follow these steps:\n",
    "\n",
    "1. **Set Up Your Flask Application**\n",
    "\n",
    "    Create a Python file (e.g., `app.py`):\n",
    "\n",
    "    ```python\n",
    "    from flask import Flask, request, redirect, url_for, render_template, send_from_directory\n",
    "    import os\n",
    "    from werkzeug.utils import secure_filename\n",
    "\n",
    "    app = Flask(__name__)\n",
    "    app.config['SECRET_KEY'] = 'your_secret_key'\n",
    "    app.config['UPLOAD_FOLDER'] = 'uploads/'  # Directory to save uploaded files\n",
    "    app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # Maximum file size (16 MB)\n",
    "\n",
    "    # Ensure the upload folder exists\n",
    "    os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n",
    "\n",
    "    @app.route('/', methods=['GET', 'POST'])\n",
    "    def upload_file():\n",
    "        if request.method == 'POST':\n",
    "            if 'file' not in request.files:\n",
    "                return 'No file part'\n",
    "            file = request.files['file']\n",
    "            if file.filename == '':\n",
    "                return 'No selected file'\n",
    "            if file and allowed_file(file.filename):\n",
    "                filename = secure_filename(file.filename)\n",
    "                file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
    "                return redirect(url_for('upload_file'))\n",
    "        return render_template('upload.html')\n",
    "\n",
    "    @app.route('/uploads/<filename>')\n",
    "    def uploaded_file(filename):\n",
    "        return send_from_directory(app.config['UPLOAD_FOLDER'], filename)\n",
    "\n",
    "    def allowed_file(filename):\n",
    "        ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}\n",
    "        return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        app.run(debug=True)\n",
    "    ```\n",
    "\n",
    "2. **Create HTML Form**\n",
    "\n",
    "    Create `templates/upload.html`:\n",
    "\n",
    "    ```html\n",
    "    <!doctype html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <title>Upload File</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Upload File</h1>\n",
    "        <form method=\"POST\" enctype=\"multipart/form-data\">\n",
    "            <input type=\"file\" name=\"file\"><br><br>\n",
    "            <input type=\"submit\" value=\"Upload\">\n",
    "        </form>\n",
    "    </body>\n",
    "    </html>\n",
    "    ```\n",
    "\n",
    "3. **Handle File Uploads**\n",
    "\n",
    "    - **Check for File:** Verify the presence of the file.\n",
    "    - **Check Filename:** Ensure the filename is not empty.\n",
    "    - **Validate File Type:** Ensure the file type is allowed.\n",
    "    - **Save File:** Use `file.save()` to store the file.\n",
    "    - **Redirect or Respond:** Redirect or send a response post-upload.\n",
    "\n",
    "4. **Define Helper Functions**\n",
    "\n",
    "    - **`allowed_file(filename)`:** Check for allowed file extensions.\n",
    "    - **`secure_filename(filename)`:** Securely handle filenames.\n",
    "\n",
    "5. **Run Your Application**\n",
    "\n",
    "    Run the application with:\n",
    "\n",
    "    ```bash\n",
    "    python app.py\n",
    "    ```\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Set Up Flask:** Configure file upload settings.\n",
    "2. **Create Form:** Design the file upload form.\n",
    "3. **Handle Uploads:** Validate and save uploaded files.\n",
    "4. **Define Helpers:** Validate file types and secure filenames.\n",
    "5. **Run and Test:** Test file uploads in your application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81151085-4d7f-462a-887e-0595b7a12a52",
   "metadata": {},
   "source": [
    "# 47.Describe the steps to create a Flask blueprint and why you might use one.\n",
    "\n",
    "1. **Create a Blueprint**\n",
    "\n",
    "    Create a separate Python file (e.g., `auth.py`):\n",
    "\n",
    "    ```python\n",
    "    from flask import Blueprint, render_template, request, redirect, url_for\n",
    "\n",
    "    # Define the blueprint\n",
    "    auth = Blueprint('auth', __name__, template_folder='templates', static_folder='static')\n",
    "\n",
    "    # Define routes for the blueprint\n",
    "    @auth.route('/login', methods=['GET', 'POST'])\n",
    "    def login():\n",
    "        if request.method == 'POST':\n",
    "            # Handle login logic here\n",
    "            return redirect(url_for('auth.dashboard'))\n",
    "        return render_template('login.html')\n",
    "\n",
    "    @auth.route('/dashboard')\n",
    "    def dashboard():\n",
    "        return render_template('dashboard.html')\n",
    "    ```\n",
    "\n",
    "2. **Register the Blueprint**\n",
    "\n",
    "    In your main application file (e.g., `app.py`):\n",
    "\n",
    "    ```python\n",
    "    from flask import Flask\n",
    "    from auth import auth  # Import the blueprint\n",
    "\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    # Register the blueprint\n",
    "    app.register_blueprint(auth, url_prefix='/auth')\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        app.run(debug=True)\n",
    "    ```\n",
    "\n",
    "3. **Create Template Files**\n",
    "\n",
    "    Create the necessary template files in the `templates` directory:\n",
    "\n",
    "    **`templates/login.html`:**\n",
    "\n",
    "    ```html\n",
    "    <!doctype html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <title>Login</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Login</h1>\n",
    "        <form method=\"POST\">\n",
    "            <label for=\"username\">Username:</label>\n",
    "            <input type=\"text\" id=\"username\" name=\"username\"><br>\n",
    "            <label for=\"password\">Password:</label>\n",
    "            <input type=\"password\" id=\"password\" name=\"password\"><br>\n",
    "            <input type=\"submit\" value=\"Login\">\n",
    "        </form>\n",
    "    </body>\n",
    "    </html>\n",
    "    ```\n",
    "\n",
    "    **`templates/dashboard.html`:**\n",
    "\n",
    "    ```html\n",
    "    <!doctype html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <title>Dashboard</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Welcome to the Dashboard</h1>\n",
    "    </body>\n",
    "    </html>\n",
    "    ```\n",
    "\n",
    "4. **Create Static Files (Optional)**\n",
    "\n",
    "    Place static files (e.g., CSS, JS) in the blueprint’s `static` directory.\n",
    "    \n",
    " **Directory Structure:**   \n",
    " \n",
    "/your_application\n",
    "    /auth\n",
    "        __init__.py\n",
    "        auth.py\n",
    "        /templates\n",
    "            login.html\n",
    "            dashboard.html\n",
    "        /static\n",
    "            style.css\n",
    "    app.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b07d7-c4ac-4c4f-a6dc-42f819c62c9c",
   "metadata": {},
   "source": [
    "# What is the difference between Series & Dataframes\n",
    "\n",
    "\n",
    "| Aspect            | Series                                         | DataFrame                                      |\n",
    "|-------------------|------------------------------------------------|------------------------------------------------|\n",
    "| **Definition**    | A one-dimensional array-like object.           | A two-dimensional, tabular data structure.     |\n",
    "| **Structure**     | Consists of an index and a data array.         | Consists of rows and columns with labels.      |\n",
    "| **Use Case**      | Represents a single column or row of data.     | Represents structured data with multiple columns and rows. |\n",
    "| **Example**       | ```python                                      | ```python                                      |\n",
    "|                   | import pandas as pd                           | import pandas as pd                           |\n",
    "|                   | data = [1, 2, 3, 4]                           | data = {                                      |\n",
    "|                   | series = pd.Series(data)                      |     'Column1': [1, 2, 3, 4],                 |\n",
    "|                   | ```                                            |     'Column2': ['A', 'B', 'C', 'D']          |\n",
    "|                   |                                                | }                                              |\n",
    "|                   |                                                | dataframe = pd.DataFrame(data)                |\n",
    "|                   |                                                | ```                                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917fa10c-88df-46b6-93b4-8698db491018",
   "metadata": {},
   "source": [
    "# Create a database name Travel_Planner in mysql ,and create a table name bookings in that which having attributes (user_id INT, flight_id INT,hotel_id INT, activiy_id INT,booking_date DATE) .fill wiht some dummy value .Now you have to read the conten of this table using pandas as dataframe.Show the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54505cf8-db3f-4525-92f3-05baf89372ea",
   "metadata": {},
   "source": [
    "```python\n",
    "# Install pymysql \n",
    "# !pip install pymysql\n",
    "\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "# Define the database connection parameters\n",
    "db_config = {\n",
    "    'user': 'your_username',\n",
    "    'password': 'your_password',\n",
    "    'host': 'localhost',\n",
    "    'database': 'Travel_Planner'\n",
    "}\n",
    "\n",
    "# Connect to MySQL and create the database and table, then insert dummy values\n",
    "connection = pymysql.connect(\n",
    "    user=db_config['user'],\n",
    "    password=db_config['password'],\n",
    "    host=db_config['host']\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS Travel_Planner;\")\n",
    "cursor.execute(\"USE Travel_Planner;\")\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bookings (\n",
    "        user_id INT,\n",
    "        flight_id INT,\n",
    "        hotel_id INT,\n",
    "        activiy_id INT,\n",
    "        booking_date DATE\n",
    "    );\n",
    "\"\"\")\n",
    "cursor.execute(\"\"\"\n",
    "    INSERT INTO bookings (user_id, flight_id, hotel_id, activiy_id, booking_date) VALUES\n",
    "    (1, 101, 201, 301, '2024-07-01'),\n",
    "    (2, 102, 202, 302, '2024-07-02'),\n",
    "    (3, 103, 203, 303, '2024-07-03'),\n",
    "    (4, 104, 204, 304, '2024-07-04'),\n",
    "    (5, 105, 205, 305, '2024-07-05')\n",
    "    ON DUPLICATE KEY UPDATE\n",
    "        flight_id=VALUES(flight_id),\n",
    "        hotel_id=VALUES(hotel_id),\n",
    "        activiy_id=VALUES(activiy_id),\n",
    "        booking_date=VALUES(booking_date);\n",
    "\"\"\")\n",
    "connection.commit()\n",
    "\n",
    "# Query to read the contents of the bookings table\n",
    "query = \"SELECT * FROM bookings;\"\n",
    "\n",
    "# Read the data into a pandas DataFrame\n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "# Close the connection\n",
    "connection.close()\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0f2b0-4289-4022-a85d-6354313ed17f",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "1. **SQL Commands within Python**:\n",
    "    - `CREATE DATABASE IF NOT EXISTS Travel_Planner;`: Creates a new database named `Travel_Planner` if it does not already exist.\n",
    "    - `USE Travel_Planner;`: Selects the `Travel_Planner` database for subsequent operations.\n",
    "    - `CREATE TABLE IF NOT EXISTS bookings ...`: Creates a table named `bookings` with specified columns if it does not already exist.\n",
    "    - `INSERT INTO bookings ... ON DUPLICATE KEY UPDATE ...`: Inserts dummy data into the `bookings` table, updating existing records if they already exist.\n",
    "\n",
    "2. **Python Code**:\n",
    "    - **Import libraries**: Import `pandas` for data manipulation and `pymysql` for MySQL connection.\n",
    "    - **Database connection parameters**: Define the connection parameters (`user`, `password`, `host`, `database`).\n",
    "    - **Create a connection**: Use `pymysql.connect` to establish a connection to the MySQL database.\n",
    "    - **Execute SQL commands**: Use `cursor.execute` to run SQL commands for creating the database and table and inserting data.\n",
    "    - **Read data into DataFrame**: Use `pd.read_sql` to execute the query and read the data into a pandas DataFrame.\n",
    "    - **Close the connection**: Close the connection to the database to free resources.\n",
    "    - **Display the DataFrame**: Display the contents of the DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810fd86-aa11-4ab5-a9b0-3c2d53258b20",
   "metadata": {},
   "source": [
    "# Difference between loc and iloc\n",
    "\n",
    "| Feature                    | `.loc`                                       | `.iloc`                                       |\n",
    "|----------------------------|----------------------------------------------|-----------------------------------------------|\n",
    "| **Indexing Type**          | Label-based                                  | Integer-based                                 |\n",
    "| **Endpoint Inclusion**     | Includes the endpoint in slicing             | Excludes the endpoint in slicing              |\n",
    "| **Supports Labels**        | Yes                                          | No                                            |\n",
    "| **Supports Integer Index** | No                                           | Yes                                           |\n",
    "| **Boolean Arrays**         | Supported                                    | Not supported                                 |\n",
    "| **Single Row Selection**   | `data.loc['row_label']`                      | `data.iloc[row_index]`                        |\n",
    "| **Multiple Rows Selection**| `data.loc[['row_label1', 'row_label2']]`     | `data.iloc[[row_index1, row_index2]]`         |\n",
    "| **Range of Rows Selection**| `data.loc['start_label':'end_label']`        | `data.iloc[start_index:end_index]`            |\n",
    "| **Single Cell Selection**  | `data.loc['row_label', 'col_label']`         | `data.iloc[row_index, col_index]`             |\n",
    "| **Multiple Cells Selection**| `data.loc[['row_label1', 'row_label2'], ['col_label1', 'col_label2']]` | `data.iloc[[row_index1, row_index2], [col_index1, col_index2]]` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0278ca5-ebd1-4c53-a00b-c021b40dc4ea",
   "metadata": {},
   "source": [
    "# What is the difference between supervised and unsupervised learning?\n",
    "\n",
    "| Feature                      | Supervised Learning                              | Unsupervised Learning                          |\n",
    "|------------------------------|--------------------------------------------------|------------------------------------------------|\n",
    "| **Definition**               | Learning from labeled data                       | Learning from unlabeled data                   |\n",
    "| **Goal**                     | Predict outcomes for new data                    | Find hidden patterns or intrinsic structures   |\n",
    "| **Data**                     | Labeled (input-output pairs)                     | Unlabeled (only input data)                    |\n",
    "| **Algorithms**               | Regression, Classification                       | Clustering, Association, Dimensionality Reduction |\n",
    "| **Examples**                 | Linear Regression, Logistic Regression, Decision Trees, Support Vector Machines, Neural Networks | K-means Clustering, Hierarchical Clustering, Apriori Algorithm, Principal Component Analysis (PCA) |\n",
    "| **Applications**             | Spam detection, Sentiment analysis, Disease prediction, Price prediction | Customer segmentation, Anomaly detection, Market basket analysis, Feature extraction |\n",
    "| **Output**                   | Discrete labels (classification) or continuous values (regression) | Clusters, Association rules, Reduced dimensions |\n",
    "| **Feedback**                 | Direct (from labels)                             | Indirect (from the data structure)             |\n",
    "| **Training Process**         | Uses labeled data to learn the mapping function  | Uses input data to identify patterns or groupings |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705a7d3-ae1c-46e9-856d-e5edca4d827e",
   "metadata": {},
   "source": [
    "# Explain the bias-variance tradeoff\n",
    "\n",
    "### Bias\n",
    "\n",
    "- **Definition**: Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. It represents the difference between the average prediction of our model and the true values.\n",
    "- **Impact**: High bias can cause a model to underfit the data, meaning it is too simple to capture the underlying patterns of the data. This leads to systematic errors and poor performance on both training and test datasets.\n",
    "- **Example**: Using a linear model to fit data that has a non-linear relationship.\n",
    "\n",
    "### Variance\n",
    "\n",
    "- **Definition**: Variance refers to the error introduced by the model's sensitivity to the fluctuations in the training data. It represents how much the predictions for a given point vary between different training sets.\n",
    "- **Impact**: High variance can cause a model to overfit the data, meaning it captures noise or random fluctuations in the training data rather than the underlying pattern. This leads to excellent performance on the training dataset but poor generalization to new, unseen data.\n",
    "- **Example**: Using a very complex model with many parameters (like a high-degree polynomial) that fits the training data too closely.\n",
    "\n",
    "### Tradeoff\n",
    "\n",
    "- **Balancing Act**: The bias-variance tradeoff is about finding the right balance between bias and variance to minimize the total error. The goal is to choose a model that generalizes well to new data while avoiding both underfitting and overfitting.\n",
    "- **Visualization**: In practice, this tradeoff is often visualized as a U-shaped curve where the total error is plotted as a function of model complexity. The total error is the sum of bias squared, variance, and irreducible error.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Underfitting**: High bias, low variance. The model is too simple.\n",
    "- **Overfitting**: Low bias, high variance. The model is too complex.\n",
    "- **Optimal Model**: The point where bias and variance are balanced to achieve the minimum total error.\n",
    "\n",
    "### Practical Tips\n",
    "\n",
    "1. **Cross-Validation**: Use techniques like cross-validation to assess how well your model generalizes to new data.\n",
    "2. **Regularization**: Apply regularization techniques to reduce model complexity and avoid overfitting.\n",
    "3. **Model Selection**: Choose models that are appropriate for the complexity of your data and problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79366cb1-32bf-4e28-abae-6157b7c53fec",
   "metadata": {},
   "source": [
    "# What are precision nd recall? How are they different from accurcay?\n",
    "\n",
    "### Precision\n",
    "\n",
    "- **Definition**: Precision is the ratio of true positive predictions to the total number of positive predictions made (i.e., true positives + false positives).\n",
    "- **Formula**: \\[ \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} \\]\n",
    "- **Purpose**: Measures how many of the predicted positives are actually positive.\n",
    "\n",
    "### Recall\n",
    "\n",
    "- **Definition**: Recall (or Sensitivity) is the ratio of true positive predictions to the total number of actual positives (i.e., true positives + false negatives).\n",
    "- **Formula**: \\[ \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} \\]\n",
    "- **Purpose**: Measures how many of the actual positives were correctly predicted.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "- **Definition**: Accuracy is the ratio of all correct predictions (true positives + true negatives) to the total number of predictions.\n",
    "- **Formula**: \\[ \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Predictions}} \\]\n",
    "- **Purpose**: Measures the overall correctness of the model.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "- **Precision** focuses on the quality of positive predictions, while **recall** focuses on the quantity of actual positives identified.\n",
    "- **Accuracy** provides a general measure of correctness but may be misleading in imbalanced datasets where one class is more frequent than the other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bf6145-165d-47bf-9187-4d26ca279c7e",
   "metadata": {},
   "source": [
    "# What is overfiting and how can it be prevented?\n",
    "\n",
    "- **Overfitting** occurs when a model learns the details and noise in the training data to the extent that it negatively impacts its performance on new, unseen data. This results in a model that performs well on training data but poorly on test data.\n",
    "\n",
    "### Causes\n",
    "- **Excessive Complexity**: Using a model that is too complex relative to the amount of training data (e.g., a high-degree polynomial in regression).\n",
    "- **Insufficient Data**: Having too little training data can lead to overfitting because the model may memorize the data rather than generalize from it.\n",
    "\n",
    "### Prevention Techniques\n",
    "1. **Cross-Validation**: Use techniques like k-fold cross-validation to ensure the model performs well on different subsets of the data.\n",
    "2. **Regularization**: Apply regularization methods (e.g., L1 or L2 regularization) to penalize large coefficients and reduce model complexity.\n",
    "3. **Pruning**: For decision trees, pruning techniques can help reduce the complexity of the model.\n",
    "4. **Early Stopping**: In iterative algorithms like gradient descent, monitor performance on a validation set and stop training when performance starts to degrade.\n",
    "5. **Simplify the Model**: Use simpler models with fewer parameters or features.\n",
    "6. **Increase Training Data**: Provide more training examples to help the model learn more general patterns.\n",
    "7. **Ensemble Methods**: Combine multiple models to improve generalization (e.g., bagging, boosting).\n",
    "\n",
    "### Summary\n",
    "* Overfitting is when a model performs well on training data but poorly on unseen data. Preventing it involves techniques to ensure the model generalizes well by managing complexity, using regularization, and validating performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ec6aa-e119-4fb2-b993-12ae7ff57a8d",
   "metadata": {},
   "source": [
    "# Explain the concept of cross-validation\n",
    "\n",
    "- **Cross-validation** is a technique used to evaluate the performance of a machine learning model and ensure that it generalizes well to new, unseen data. It involves dividing the data into multiple subsets or \"folds\" and systematically training and validating the model on different combinations of these folds.\n",
    "\n",
    "### Common Types\n",
    "\n",
    "1. **K-Fold Cross-Validation**\n",
    "   - **Process**: The data is split into `k` equally sized folds. The model is trained on `k-1` folds and validated on the remaining fold. This process is repeated `k` times, each time with a different fold as the validation set.\n",
    "   - **Advantages**: Provides a robust estimate of model performance by averaging results across different folds.\n",
    "   - **Example**: 10-fold cross-validation splits the data into 10 folds, resulting in 10 different training and validation sets.\n",
    "\n",
    "2. **Leave-One-Out Cross-Validation (LOOCV)**\n",
    "   - **Process**: Each data point is used once as the validation set while the remaining data points are used for training. This means the number of folds equals the number of data points.\n",
    "   - **Advantages**: Uses almost all data for training, which can be beneficial for small datasets.\n",
    "   - **Disadvantages**: Computationally expensive for large datasets.\n",
    "\n",
    "3. **Stratified K-Fold Cross-Validation**\n",
    "   - **Process**: Similar to k-fold cross-validation but ensures that each fold maintains the same proportion of class labels as the original dataset.\n",
    "   - **Advantages**: Useful for imbalanced datasets where some classes are underrepresented.\n",
    "\n",
    "### Advantages\n",
    "- **Model Evaluation**: Provides a more reliable estimate of model performance compared to a single train-test split.\n",
    "- **Variance Reduction**: Reduces the variance in the performance estimate by averaging results over multiple folds.\n",
    "\n",
    "### Summary\n",
    "* Cross-validation is a technique for assessing how well a model generalizes to unseen data by dividing the dataset into multiple folds, training and validating the model across these folds, and averaging the performance metrics to get a robust evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3285b8-aaac-4cd7-9c40-2af5fbf2ca63",
   "metadata": {},
   "source": [
    "# What is the difference between a classification and a regression problem\n",
    "\n",
    "### Classification\n",
    "\n",
    "- **Definition**: Predicts discrete labels or categories.\n",
    "- **Output**: Categorical (e.g., spam or not spam).\n",
    "- **Examples**: Email classification, image recognition.\n",
    "- **Metrics**: Accuracy, precision, recall.\n",
    "\n",
    "### Regression\n",
    "\n",
    "- **Definition**: Predicts continuous values.\n",
    "- **Output**: Numerical (e.g., house price).\n",
    "- **Examples**: Price prediction, temperature forecasting.\n",
    "- **Metrics**: Mean Squared Error (MSE), R-squared.\n",
    "\n",
    "### Key Difference\n",
    "- **Classification**: Output is discrete categories.\n",
    "- **Regression**: Output is continuous values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538afa0d-c9d1-4f61-a363-28fe1a198b2b",
   "metadata": {},
   "source": [
    "# Explain the concept of ensemble learning\"\n",
    "\n",
    "- **Ensemble Learning** combines multiple models to improve performance and robustness.\n",
    "\n",
    "### Key Methods\n",
    "1. **Bagging**: Trains multiple models on different data subsets and combines their predictions.\n",
    "   - **Example**: Random Forest.\n",
    "\n",
    "2. **Boosting**: Sequentially trains models to correct errors of previous ones.\n",
    "   - **Example**: Gradient Boosting.\n",
    "\n",
    "3. **Stacking**: Combines multiple models using a meta-model to improve predictions.\n",
    "   - **Example**: Stacked Generalization.\n",
    "\n",
    "### Advantages\n",
    "- **Better Accuracy**: Typically more accurate than individual models.\n",
    "- **Reduced Overfitting**: Helps reduce overfitting and errors.\n",
    "\n",
    "### Summary\n",
    "* Ensemble learning improves model performance by combining predictions from multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938db4d-5b45-4319-817c-9d6aff2455bd",
   "metadata": {},
   "source": [
    "# What is gradient descent and how does it work\n",
    "- **Gradient Descent** is an optimization algorithm used to minimize a loss function by updating model parameters.\n",
    "\n",
    "### How It Works\n",
    "1. **Initialize Parameters**: Start with initial values.\n",
    "2. **Compute Gradient**: Calculate how the loss function changes with parameters.\n",
    "3. **Update Parameters**: Adjust parameters to reduce the loss.\n",
    "   - **Update Rule**: \\[ \\theta = \\theta - \\eta \\cdot \\nabla L(\\theta) \\]\n",
    "4. **Iterate**: Repeat until convergence.\n",
    "\n",
    "### Key Points\n",
    "- **Learning Rate (\\(\\eta\\))**: Controls step size. Too large or too small affects convergence.\n",
    "\n",
    "### Summary\n",
    "* Gradient Descent minimizes the loss function by iteratively adjusting model parameters based on gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48b1f2-bf4e-44d9-8788-638fa4890b1c",
   "metadata": {},
   "source": [
    "# Describe the difference between batch gradient descent and stochastic gradient descent\"\n",
    "\n",
    "### Batch Gradient Descent\n",
    "\n",
    "- **Definition**: Computes gradients using the entire dataset in one go.\n",
    "- **Pros**: Stable updates.\n",
    "- **Cons**: Slow and memory-intensive for large datasets.\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "- **Definition**: Computes gradients using one example (or small batch) at a time.\n",
    "- **Pros**: Faster and handles large datasets better.\n",
    "- **Cons**: Noisy updates, less stable.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Batch**: Full dataset, stable but slow.\n",
    "- **SGD**: Single examples, faster but noisier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8ef190-fffe-46b8-a982-4c42ca02b6dc",
   "metadata": {},
   "source": [
    "# What is the curse of dimensionality in machine learning\n",
    "\n",
    "- **Definition**: Problems with high-dimensional data, such as increased complexity and sparsity.\n",
    "- **Issues**:\n",
    "  - High computational cost\n",
    "  - Data sparsity\n",
    "  - Ineffective distance metrics\n",
    "  - Risk of overfitting\n",
    "- **Solutions**:\n",
    "  - Dimensionality reduction (e.g., PCA)\n",
    "  - Feature selection\n",
    "\n",
    "### Summary\n",
    "* High dimensions lead to complexity and sparsity issues. Dimensionality reduction and feature selection can help.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a171a9ab-5b0a-4a1c-b734-2e2c785994ad",
   "metadata": {},
   "source": [
    "# Explain the difference beteen L1 and L2 regularization\"\n",
    "\n",
    "### L1 Regularization (Lasso)\n",
    "\n",
    "- **Definition**: Adds the absolute value of the coefficients to the loss function.\n",
    "- **Formula**: \\[ \\text{Loss} = \\text{Original Loss} + \\lambda \\sum_{i} |\\theta_i| \\]\n",
    "- **Effect**: Can drive some coefficients to exactly zero, performing feature selection.\n",
    "- **Usage**: Useful when you want sparse models with fewer features.\n",
    "\n",
    "### L2 Regularization (Ridge)\n",
    "\n",
    "- **Definition**: Adds the squared value of the coefficients to the loss function.\n",
    "- **Formula**: \\[ \\text{Loss} = \\text{Original Loss} + \\lambda \\sum_{i} \\theta_i^2 \\]\n",
    "- **Effect**: Shrinks coefficients but does not necessarily make them zero. Helps with multicollinearity.\n",
    "- **Usage**: Useful when you want to reduce model complexity without completely removing features.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **L1 Regularization**: Adds absolute values, can zero out coefficients (sparse model).\n",
    "- **L2 Regularization**: Adds squared values, shrinks coefficients (less sparse model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94d08f-47d7-4790-aae0-12cc26221185",
   "metadata": {},
   "source": [
    "# What is a confusion matrix and how is it used\n",
    "- A **Confusion Matrix** is a table used to evaluate the performance of a classification model. It compares the predicted labels to the actual labels, providing a detailed breakdown of classification results.\n",
    "\n",
    "### Structure\n",
    "\n",
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|---------------------|---------------------|\n",
    "| **Actual Positive** | True Positive (TP)   | False Negative (FN)  |\n",
    "| **Actual Negative** | False Positive (FP)  | True Negative (TN)   |\n",
    "\n",
    "### Components\n",
    "\n",
    "- **True Positive (TP)**: Correctly predicted positive cases.\n",
    "- **True Negative (TN)**: Correctly predicted negative cases.\n",
    "- **False Positive (FP)**: Incorrectly predicted positive cases (Type I error).\n",
    "- **False Negative (FN)**: Incorrectly predicted negative cases (Type II error).\n",
    "\n",
    "### Uses\n",
    "\n",
    "- **Performance Metrics**: Derives metrics like accuracy, precision, recall, and F1-score.\n",
    "  - **Accuracy**: \\[ \\frac{TP + TN}{TP + TN + FP + FN} \\]\n",
    "  - **Precision**: \\[ \\frac{TP}{TP + FP} \\]\n",
    "  - **Recall**: \\[ \\frac{TP}{TP + FN} \\]\n",
    "  - **F1-Score**: \\[ 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\]\n",
    "\n",
    "### Summary\n",
    "\n",
    "* A confusion matrix shows the counts of true and false positives and negatives, helping to evaluate the performance of a classification model through various metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a0cd81-553f-4be2-be89-78d0f97f7313",
   "metadata": {},
   "source": [
    "# Define AUC-ROC curve\"\n",
    "\n",
    "- **ROC Curve (Receiver Operating Characteristic Curve)**: A graphical plot that illustrates the performance of a classification model across different threshold values. It shows the trade-off between the True Positive Rate (TPR) and the False Positive Rate (FPR).\n",
    "\n",
    "### Components\n",
    "\n",
    "- **True Positive Rate (TPR)**: Also known as Recall or Sensitivity, it measures the proportion of actual positives correctly identified.\n",
    "  - **Formula**: \\[ \\text{TPR} = \\frac{TP}{TP + FN} \\]\n",
    "- **False Positive Rate (FPR)**: Measures the proportion of actual negatives incorrectly classified as positives.\n",
    "  - **Formula**: \\[ \\text{FPR} = \\frac{FP}{FP + TN} \\]\n",
    "\n",
    "### AUC (Area Under the Curve)\n",
    "\n",
    "- **Definition**: The area under the ROC curve, which quantifies the overall ability of the model to discriminate between positive and negative classes.\n",
    "- **Range**: \n",
    "  - **0.5**: No discriminative power (random model).\n",
    "  - **1.0**: Perfect model.\n",
    "\n",
    "### Uses\n",
    "\n",
    "- **Model Comparison**: AUC-ROC is used to compare the performance of different classification models.\n",
    "- **Threshold Selection**: Helps in choosing the best threshold for classification by analyzing the trade-offs between TPR and FPR.\n",
    "\n",
    "### Summary\n",
    "\n",
    "* The ROC curve plots TPR against FPR across thresholds, and the AUC measures the model's ability to distinguish between classes. Higher AUC indicates better model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490aedcc-ea54-4090-be07-e940eb50954d",
   "metadata": {},
   "source": [
    "# Explain the k-nearest neighbors algorithm\"\n",
    "- **KNN** is a simple algorithm used for classification and regression by finding the `k` nearest neighbors to a data point.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Choose `k`**: Decide how many neighbors to consider.\n",
    "2. **Calculate Distances**: Find distances between the query point and all training points.\n",
    "3. **Find Neighbors**: Identify the `k` closest points.\n",
    "4. **Make Prediction**:\n",
    "   - **Classification**: Majority class of the `k` neighbors.\n",
    "   - **Regression**: Average of the `k` neighbors’ values.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Simple and Intuitive**\n",
    "- **No Training Phase**\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- **Computationally Intensive**\n",
    "- **Sensitive to `k` and Features**\n",
    "\n",
    "### Summary\n",
    "* KNN classifies or predicts based on the `k` nearest data points but can be slow and sensitive to the choice of `k`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40cc24f-4c01-4b0e-8442-d8798b30838a",
   "metadata": {},
   "source": [
    "# Explain the basic concept of a Support Vector Machine (SVM)\"\n",
    "- **SVM** is a classification algorithm that finds the best hyperplane to separate classes with the maximum margin.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Find Hyperplane**: Determines the optimal hyperplane that maximizes the margin between classes.\n",
    "2. **Support Vectors**: Key data points closest to the hyperplane that affect its position.\n",
    "3. **Classification**: Classifies new points based on their position relative to the hyperplane.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Linear SVM**: For linearly separable data.\n",
    "- **Non-Linear SVM**: Uses kernels for complex data.\n",
    "\n",
    "### Summary\n",
    "* SVM creates a hyperplane to separate classes with the largest margin and can handle both linear and non-linear data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a3f5b1-16a1-4eaa-b185-fa346bb47b16",
   "metadata": {},
   "source": [
    "# How does the kernel trick work in SVM\n",
    "- **Kernel Trick** is a technique used in SVM to handle non-linearly separable data by transforming it into a higher-dimensional space where it can become linearly separable.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Transform Data**: Maps the original data into a higher-dimensional space using a kernel function.\n",
    "2. **Apply Linear SVM**: Finds a hyperplane in this higher-dimensional space that separates the data.\n",
    "3. **Kernel Function**: Computes the dot product in the transformed space without explicitly transforming the data. Common kernels include:\n",
    "   - **Polynomial Kernel**: \\[ K(x, x') = (x \\cdot x' + c)^d \\]\n",
    "   - **Radial Basis Function (RBF) Kernel**: \\[ K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2) \\]\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Handles Non-Linearity**: Allows SVM to perform well on complex datasets that are not linearly separable.\n",
    "- **Computational Efficiency**: Avoids the need for explicit transformation, which can be computationally expensive.\n",
    "\n",
    "### Summary\n",
    "* The kernel trick enables SVM to classify non-linearly separable data by implicitly mapping it to a higher-dimensional space and applying linear separation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7f92c7-5377-4bb7-9ada-cf3153b63dcd",
   "metadata": {},
   "source": [
    "# What are the different types of kernels used in SVM and when would you use each\n",
    "## Types of Kernels in SVM\n",
    "\n",
    "### 1. Linear Kernel\n",
    "\n",
    "- **Formula**: \\[ K(x, x') = x \\cdot x' \\]\n",
    "- **Use When**: The data is linearly separable. It is the simplest and fastest kernel, suitable for problems where the classes can be separated by a straight line or hyperplane.\n",
    "\n",
    "### 2. Polynomial Kernel\n",
    "\n",
    "- **Formula**: \\[ K(x, x') = (x \\cdot x' + c)^d \\]\n",
    "- **Parameters**: `c` (constant), `d` (degree of the polynomial).\n",
    "- **Use When**: The data is not linearly separable but can be separated by a polynomial decision boundary. Useful for capturing interactions between features.\n",
    "\n",
    "### 3. Radial Basis Function (RBF) Kernel\n",
    "\n",
    "- **Formula**: \\[ K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2) \\]\n",
    "- **Parameter**: `\\gamma` (controls the spread of the kernel).\n",
    "- **Use When**: The data is complex and not linearly separable. It works well for most practical problems and can handle a wide range of data distributions.\n",
    "\n",
    "### 4. Sigmoid Kernel\n",
    "\n",
    "- **Formula**: \\[ K(x, x') = \\tanh(\\alpha (x \\cdot x') + c) \\]\n",
    "- **Parameters**: `\\alpha` (scaling factor), `c` (constant).\n",
    "- **Use When**: The data is similar to what might be encountered in neural networks. This kernel mimics the behavior of a neural network activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50919504-90d1-4263-bd57-59f5115c3c4c",
   "metadata": {},
   "source": [
    "# What is the hyperplane in SVM and how is it determined\n",
    "\n",
    "- A **hyperplane** is a decision boundary that separates different classes in the feature space. In 2D, it's a line; in 3D, it's a plane; and in higher dimensions, it's a hyperplane.\n",
    "\n",
    "### Determination\n",
    "\n",
    "1. **Objective**: Find the hyperplane that maximizes the margin between the closest data points of each class (support vectors).\n",
    "\n",
    "2. **Margin**: The distance between the hyperplane and the nearest data points from each class.\n",
    "\n",
    "3. **Mathematical Formulation**:\n",
    "   - **Hyperplane Equation**: \\[ \\mathbf{w} \\cdot \\mathbf{x} + b = 0 \\]\n",
    "     - **\\(\\mathbf{w}\\)**: Weight vector perpendicular to the hyperplane.\n",
    "     - **\\(\\mathbf{x}\\)**: Data point.\n",
    "     - **\\(b\\)**: Bias term.\n",
    "\n",
    "4. **Optimization**:\n",
    "   - **Objective Function**: Maximize the margin, which is equivalent to minimizing \\(\\frac{1}{2} \\|\\mathbf{w}\\|^2\\).\n",
    "   - **Constraints**: Ensure that data points are correctly classified with a margin of at least 1 (i.e., \\(\\mathbf{y}_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1\\)).\n",
    "\n",
    "5. **Support Vectors**:\n",
    "   - Data points that lie closest to the hyperplane and define the margin. They are critical in determining the position of the hyperplane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66864bc3-ca60-4578-9f03-22afd34363cc",
   "metadata": {},
   "source": [
    "# What are the pros and cons of using a Support Vector Machine (SVM)\n",
    "\n",
    "| **Pros**                                        | **Cons**                                      |\n",
    "|-------------------------------------------------|-----------------------------------------------|\n",
    "| **Effective in High Dimensions**                | **Computationally Intensive**                 |\n",
    "| Performs well with high-dimensional data.      | Training can be slow and memory-intensive.   |\n",
    "| **Robust to Overfitting**                       | **Sensitive to Choice of Kernel and Parameters** |\n",
    "| Less prone to overfitting, especially with the kernel trick. | Performance depends on kernel and hyperparameter tuning. |\n",
    "| **Versatile with Kernels**                     | **Not Suitable for Large Datasets**           |\n",
    "| Can handle both linear and non-linear data with different kernels. | Can be impractical for very large datasets due to high cost. |\n",
    "| **Clear Margin of Separation**                  | **Difficult to Interpret**                    |\n",
    "| Finds the hyperplane with the maximum margin, which helps with generalization. | Model and decision boundaries can be complex and hard to interpret. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "* SVMs are powerful for high-dimensional and complex problems but can be computationally expensive and challenging to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3214e67-75f5-4da4-88fb-3a7125f178e8",
   "metadata": {},
   "source": [
    "# Explain the difference between a hard margin and a soft margin SVM\"\n",
    "\n",
    "| **Aspect**               | **Hard Margin SVM**                              | **Soft Margin SVM**                              |\n",
    "|--------------------------|--------------------------------------------------|--------------------------------------------------|\n",
    "| **Definition**           | SVM with a strict requirement for no misclassification. | SVM allows some misclassification to handle noisy data. |\n",
    "| **Margin**               | Requires all data points to be correctly classified with a clear margin. | Allows some points to be within the margin or misclassified. |\n",
    "| **Objective**            | Maximize the margin while ensuring perfect classification. | Maximize the margin while allowing for some errors (controlled by a penalty parameter). |\n",
    "| **Handling Noise**       | Not suitable for noisy data or overlapping classes. | More robust to noisy data and overlapping classes. |\n",
    "| **Optimization**         | Strict constraints may lead to overfitting.      | Includes a regularization parameter (C) to balance margin size and classification error. |\n",
    "| **Regularization**       | No regularization term for error tolerance.     | Uses a regularization term to control the trade-off between margin size and classification errors. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Hard Margin SVM**: No misclassification allowed; best for clean, linearly separable data.\n",
    "- **Soft Margin SVM**: Allows some misclassification; better for handling noisy or overlapping data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68ef6c-53d6-481c-a601-8f6f5c419d6c",
   "metadata": {},
   "source": [
    "# Describe the process of constructing a decision tree\"\n",
    "\n",
    "| **Step**                   | **Description**                                                            |\n",
    "|----------------------------|----------------------------------------------------------------------------|\n",
    "| **1. Data Preparation**    | Prepare and clean the dataset, ensuring it is suitable for training the tree. |\n",
    "| **2. Choose Splitting Criteria** | Select a criterion for splitting nodes (e.g., Gini impurity, Information Gain). |\n",
    "| **3. Split Data**          | Divide the data based on the selected criterion to create child nodes.      |\n",
    "| **4. Recursive Splitting** | Apply the splitting process recursively to each child node until stopping criteria are met. |\n",
    "| **5. Stopping Criteria**   | Determine when to stop splitting (e.g., maximum depth, minimum samples per leaf, or no further gain). |\n",
    "| **6. Create Terminal Nodes** | Assign a class label (for classification) or a value (for regression) to the terminal nodes. |\n",
    "| **7. Pruning (Optional)**  | Remove branches that have little importance to reduce overfitting and improve generalization. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "Constructing a decision tree involves preparing data, selecting a splitting criterion, recursively splitting the data, and applying stopping criteria to create terminal nodes. Pruning may also be used to enhance model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4a332-6863-4f37-986d-dcee43c7a0e1",
   "metadata": {},
   "source": [
    "# Describe the working principle of a decision tree\"\n",
    "\n",
    "| **Aspect**               | **Description**                                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|\n",
    "| **1. Root Node**         | The decision tree starts with the root node, which represents the entire dataset. |\n",
    "| **2. Feature Selection** | At each node, select the feature that best splits the data based on a chosen criterion (e.g., Gini impurity, Information Gain). |\n",
    "| **3. Splitting**         | Divide the data into subsets according to the selected feature’s values, creating child nodes. |\n",
    "| **4. Recursive Splitting** | Recursively apply the same process to each child node, creating further branches until stopping criteria are met. |\n",
    "| **5. Terminal Nodes**    | When a stopping criterion is met (e.g., maximum depth, minimum samples), the node becomes a terminal (leaf) node. In classification, the leaf node represents a class label; in regression, it represents a value. |\n",
    "| **6. Decision Making**   | To make predictions, trace a path from the root node to a terminal node based on the input feature values. The terminal node's class or value is the prediction. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b6a59-eada-4000-9da5-5a2aaa8c9691",
   "metadata": {},
   "source": [
    "# What is information gain and how is it used in decision trees\n",
    "\n",
    "| **Aspect**               | **Description**                                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|\n",
    "| **Definition**           | **Information Gain** measures the reduction in entropy or impurity after splitting a dataset based on a feature. It quantifies how much information a feature contributes to making predictions. |\n",
    "| **Formula**              | \\[ \\text{Information Gain} = \\text{Entropy(Parent)} - \\text{Weighted Average Entropy(Children)} \\] |\n",
    "| **Entropy**              | A measure of uncertainty or impurity in a dataset. Entropy is defined as: \\[ \\text{Entropy} = - \\sum_{i} p_i \\log_2(p_i) \\] where \\( p_i \\) is the probability of class \\( i \\). |\n",
    "| **Usage in Decision Trees** | Information Gain is used to select the best feature to split the data at each node. The feature with the highest information gain is chosen for the split, as it provides the most significant reduction in impurity. |\n",
    "| **Process**              | 1. Calculate the entropy of the dataset before the split. <br> 2. Calculate the weighted average entropy after splitting by each feature. <br> 3. Compute the information gain for each feature. <br> 4. Choose the feature with the highest information gain for the split. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fcd0d2-0097-45ce-81c7-c6eca993fc01",
   "metadata": {},
   "source": [
    "# Explain Gini impurity and its role in decision trees\"\n",
    "\n",
    "| **Aspect**               | **Description**                                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|\n",
    "| **Definition**           | **Gini Impurity** measures the impurity of a dataset. It calculates the probability of misclassifying a randomly chosen element if it were randomly labeled according to the class distribution. |\n",
    "| **Formula**              | \\[ \\text{Gini Impurity} = 1 - \\sum_{i} p_i^2 \\] <br> where \\( p_i \\) is the proportion of class \\( i \\) in the dataset. |\n",
    "| **Range**                | The value ranges from 0 (perfectly pure, all elements are of the same class) to 0.5 (maximum impurity, classes are evenly distributed). |\n",
    "| **Usage in Decision Trees** | Gini impurity is used to evaluate the quality of a split. At each node, the decision tree algorithm calculates the Gini impurity for potential splits and chooses the one that results in the lowest impurity. |\n",
    "| **Process**              | 1. Calculate the Gini impurity of the dataset before the split. <br> 2. Calculate the weighted average Gini impurity for each potential split. <br> 3. Select the split that results in the lowest Gini impurity. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "Gini impurity measures the impurity of a dataset, with lower values indicating purer nodes. It is used in decision trees to select the best feature for splitting the data by minimizing impurity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54940553-f027-49eb-beff-4bfd03660df8",
   "metadata": {},
   "source": [
    "# What are the advantages and disadvantages of decision trees\n",
    "\n",
    "| **Aspect**               | **Advantages**                                                             | **Disadvantages**                                                         |\n",
    "|--------------------------|-----------------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
    "| **Interpretability**     | **Easy to Understand and Interpret**: Decision trees provide clear decision rules and are visually intuitive. | **Complex Trees Can Be Hard to Interpret**: Large trees can become complex and difficult to understand. |\n",
    "| **Handling Non-Linearity** | **Can Handle Non-Linear Relationships**: Effective for data with complex, non-linear relationships. | **Overfitting**: Decision trees can easily overfit the training data, especially with deep trees. |\n",
    "| **Feature Importance**   | **Feature Selection**: Can implicitly perform feature selection by focusing on the most important features. | **Instability**: Small changes in the data can lead to a completely different tree structure. |\n",
    "| **No Data Preprocessing**| **No Need for Feature Scaling**: Decision trees do not require normalization or scaling of features. | **Bias towards Features with More Levels**: Features with many levels can dominate the splits. |\n",
    "| **Handling Missing Values** | **Can Handle Missing Values**: Decision trees can handle missing values and can be used with incomplete datasets. | **Computationally Expensive for Large Datasets**: Training large trees can be slow and resource-intensive. |\n",
    "| **Versatility**          | **Versatile**: Can be used for both classification and regression tasks. | **Greedy Approach**: Decision trees use a greedy approach that may not always find the optimal solution. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef4546-903c-4eec-a03e-c915c77a635e",
   "metadata": {},
   "source": [
    "# How do random forests improve upon decision trees\n",
    "\n",
    "| **Aspect**               | **Decision Trees**                                | **Random Forests**                                |\n",
    "|--------------------------|---------------------------------------------------|---------------------------------------------------|\n",
    "| **Overfitting**          | Prone to overfitting, especially with deep trees. | Reduces overfitting by averaging multiple trees. |\n",
    "| **Variance**             | High variance; small changes in data can lead to different trees. | Low variance; aggregates predictions from many trees to reduce variability. |\n",
    "| **Stability**            | Unstable; sensitive to small changes in the training data. | More stable; combines multiple trees to provide a more consistent prediction. |\n",
    "| **Accuracy**             | May not be as accurate on unseen data.           | Often more accurate due to ensemble learning and averaging. |\n",
    "| **Feature Importance**   | Can be biased towards features with more levels. | Reduces bias and provides a more balanced view of feature importance. |\n",
    "| **Complexity**           | Single tree can be complex and hard to tune.    | Ensembles multiple trees, making it more complex but also more robust. |\n",
    "| **Training Time**        | Faster training time for a single tree.          | Longer training time due to the creation of multiple trees. |\n",
    "| **Handling of Data**     | May struggle with noisy data and missing values. | Handles noisy data and missing values better by averaging multiple trees. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf8474-798e-4996-bbb0-bf0bac9f5147",
   "metadata": {},
   "source": [
    "# How does a random forest algorithm work\n",
    "\n",
    "| **Step**                   | **Description**                                                            |\n",
    "|----------------------------|----------------------------------------------------------------------------|\n",
    "| **1. Data Sampling**       | **Bootstrap Sampling**: Create multiple subsets of the training data by sampling with replacement. |\n",
    "| **2. Tree Construction**   | **Build Decision Trees**: Construct a decision tree for each subset using only a random subset of features at each split. |\n",
    "| **3. Feature Selection**   | **Random Feature Selection**: At each node, choose a random subset of features to determine the best split, reducing correlation between trees. |\n",
    "| **4. Tree Aggregation**    | **Combine Predictions**: Aggregate the predictions from all decision trees. For classification, use majority voting; for regression, use averaging. |\n",
    "| **5. Out-of-Bag (OOB) Error** | **Evaluate Model**: Use out-of-bag samples (data not included in a particular tree's bootstrap sample) to estimate the model's performance. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "Random Forest works by creating multiple decision trees using random subsets of data and features, then aggregating their predictions to improve accuracy and robustness. It reduces overfitting and increases model stability through ensemble learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07148a53-94aa-479d-974e-8a222d40acc6",
   "metadata": {},
   "source": [
    "# What is bootstrapping in the context of random forests\n",
    "\n",
    "| **Aspect**               | **Description**                                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|\n",
    "| **Definition**           | **Bootstrapping** is a resampling technique used to create multiple subsets of the training data by sampling with replacement. |\n",
    "| **Process**              | 1. **Sample with Replacement**: Randomly select samples from the original dataset to form a bootstrap sample. Each sample is chosen with equal probability and may be selected more than once. <br> 2. **Create Multiple Samples**: Repeat the sampling process to generate multiple bootstrap samples. Each sample is used to train a different decision tree. |\n",
    "| **Purpose**              | - **Diversity**: Introduces variability among the trees in the forest, which helps in reducing overfitting and improving model generalization. <br> - **Model Robustness**: Aggregating predictions from multiple trees trained on different subsets of data enhances the stability and accuracy of the model. |\n",
    "| **Out-of-Bag (OOB) Error** | **Evaluate Performance**: Data not included in a particular bootstrap sample (out-of-bag data) can be used to estimate the model's performance. This provides an unbiased evaluation of the model. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "In Random Forests, bootstrapping involves creating multiple subsets of the training data by sampling with replacement. This technique helps in building diverse decision trees and improves the overall robustness and accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fec7d6-f886-464d-9d53-dd9a19a652f3",
   "metadata": {},
   "source": [
    "# Explain the concept of feature importance in random forests\"\n",
    "\n",
    "| **Aspect**               | **Description**                                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|\n",
    "| **Definition**           | **Feature Importance** measures the contribution of each feature to the prediction accuracy of the model. |\n",
    "| **Calculation**          | 1. **Tree-Level Importance**: For each decision tree in the forest, calculate how much a feature improves the decision-making process (e.g., through reduction in impurity or increase in Information Gain). <br> 2. **Aggregate Importance**: Average the feature importance scores across all trees in the forest to get the overall importance for each feature. |\n",
    "| **Metrics Used**         | - **Gini Importance (Mean Decrease in Impurity)**: Measures the total reduction in Gini impurity brought by a feature across all nodes where it is used. <br> - **Mean Decrease in Accuracy**: Measures the decrease in model accuracy when the feature’s values are randomly permuted. |\n",
    "| **Purpose**              | - **Feature Selection**: Helps in identifying the most relevant features for the prediction task, which can be useful for reducing dimensionality and improving model performance. <br> - **Model Interpretation**: Provides insights into the importance of different features and their impact on the predictions. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "Feature importance in Random Forests quantifies the contribution of each feature to the model’s predictions. It is calculated by averaging scores from individual trees and helps in feature selection and model interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4b177-0be6-4c8c-b794-573ee48b1a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ae13ecc-35e1-4a34-a366-05adbab90a48",
   "metadata": {},
   "source": [
    "# What are the key hyperparameters of a random forest and ho do they affect the model\n",
    "\n",
    "| **Hyperparameter**       | **Description**                                                            | **Effect on Model**                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **n_estimators**         | Number of decision trees in the forest.                                     | More trees generally improve model performance and stability but increase computation time. |\n",
    "| **max_depth**            | Maximum depth of each decision tree.                                        | Controls the maximum number of levels in each tree. Deeper trees can model more complex patterns but may lead to overfitting. |\n",
    "| **min_samples_split**    | Minimum number of samples required to split an internal node.               | Higher values prevent the model from learning overly specific patterns, reducing overfitting. |\n",
    "| **min_samples_leaf**     | Minimum number of samples required to be at a leaf node.                    | Ensures that leaf nodes have a minimum number of samples, which helps in smoothing the model and avoiding overfitting. |\n",
    "| **max_features**         | Number of features to consider when looking for the best split.             | Limits the number of features considered for each split, which helps in reducing correlation between trees and improving generalization. |\n",
    "| **bootstrap**            | Whether to use bootstrap sampling (sampling with replacement).              | If True, each tree is trained on a different bootstrap sample. If False, all trees are trained on the entire dataset. |\n",
    "| **criterion**            | Function to measure the quality of a split (e.g., Gini impurity, Information Gain). | Determines how the decision trees are built; affects the splits and overall model performance. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "Key hyperparameters of a Random Forest include the number of trees (`n_estimators`), maximum tree depth (`max_depth`), minimum samples required to split or be at a leaf node (`min_samples_split`, `min_samples_leaf`), the number of features considered for splitting (`max_features`), whether to use bootstrap sampling (`bootstrap`), and the criterion for measuring splits (`criterion`). Adjusting these hyperparameters can significantly impact model performance, generalization, and training time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53c7580-933f-4f99-9bc7-91e8c3f24064",
   "metadata": {},
   "source": [
    "# Describe the logistic regression model and its assumptions\"\n",
    "\n",
    "## Logistic Regression Model\n",
    "\n",
    "| **Aspect**               | **Description**                                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|\n",
    "| **Definition**           | **Logistic Regression** is a statistical model used for binary classification. It predicts the probability of a binary outcome based on one or more predictor variables. |\n",
    "| **Model**                | The model estimates the probability that a given input belongs to a particular class using the logistic function (sigmoid function). |\n",
    "| **Equation**             | \\[ p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n)}} \\] <br> where \\( p \\) is the probability of the positive class, \\( \\beta \\) are the coefficients, and \\( x \\) are the predictor variables. |\n",
    "| **Output**               | Outputs a probability value between 0 and 1, which can be thresholded to make a binary classification decision. |\n",
    "\n",
    "## Assumptions of Logistic Regression\n",
    "\n",
    "| **Assumption**           | **Description**                                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|\n",
    "| **Linearity of Logits**  | The log-odds of the dependent variable is a linear combination of the independent variables. |\n",
    "| **Independence of Errors** | The observations are independent of each other. No autocorrelation or temporal correlation. |\n",
    "| **No Multicollinearity** | Predictor variables should not be too highly correlated with each other. High correlation can lead to multicollinearity issues. |\n",
    "| **Homoscedasticity**     | The variance of errors should be constant across all levels of the independent variables. |\n",
    "| **Large Sample Size**    | Logistic regression typically requires a larger sample size to ensure the robustness of the model. |\n",
    "| **Outcome Variable**     | The outcome variable must be binary (i.e., two possible classes). |\n",
    "\n",
    "### Summary\n",
    "\n",
    "Logistic Regression is used for binary classification by estimating probabilities via the logistic function. It assumes linearity in the log-odds, independence of observations, no multicollinearity among predictors, homoscedasticity, a large sample size, and a binary outcome variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac14cb2-e637-4388-9217-404cc752a815",
   "metadata": {},
   "source": [
    "# How does logistic regression handle binary classification problems\n",
    "\n",
    "| **Aspect**               | **Description**                                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|\n",
    "| **Objective**           | **Binary Classification**: Predicts the probability of an outcome belonging to one of two classes. |\n",
    "| **Logistic Function**    | Uses the **sigmoid function** (logistic function) to model the probability of the positive class. <br> \\[ p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n)}} \\] |\n",
    "| **Probability Output**   | The logistic function outputs a probability value between 0 and 1, indicating the likelihood of the positive class. |\n",
    "| **Thresholding**         | To make a binary decision, a threshold (typically 0.5) is applied to the probability. If the probability is greater than or equal to the threshold, the prediction is for the positive class; otherwise, it is for the negative class. |\n",
    "| **Model Fitting**        | The model is trained by estimating the coefficients (\\(\\beta\\)) that maximize the likelihood of the observed data. This is done using optimization techniques like Maximum Likelihood Estimation (MLE). |\n",
    "| **Decision Boundary**    | The decision boundary is the set of points where the probability of the positive class is equal to the threshold (usually 0.5). It is determined by the linear combination of the predictor variables and their coefficients. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "Logistic Regression handles binary classification by modeling the probability of an outcome belonging to a positive class using the logistic function. It makes predictions based on a probability threshold and estimates model coefficients to fit the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6445ba0-913c-433d-8c4a-c17cf077f9ac",
   "metadata": {},
   "source": [
    "# What is the sigmoid function and how is it used in logistic regression\n",
    "\n",
    "| **Aspect**               | **Description**                                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|\n",
    "| **Definition**           | The **sigmoid function** (logistic function) is a mathematical function that maps any real-valued number into the range [0, 1]. |\n",
    "| **Formula**              | \\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\] <br> where \\( z \\) is the input to the function (a linear combination of features and coefficients). |\n",
    "| **Output Range**         | Maps the input to a probability value between 0 and 1. |\n",
    "| **Use in Logistic Regression** | **Probability Estimation**: In logistic regression, the sigmoid function converts the linear combination of input features into a probability that indicates the likelihood of the positive class. <br> **Decision Making**: This probability is used to make binary classification decisions by applying a threshold (typically 0.5). |\n",
    "| **Role in Model**        | The sigmoid function helps in predicting the probability of class membership, allowing the model to output probabilities that can be interpreted as confidence levels for the classification. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "The sigmoid function in logistic regression maps the linear combination of features to a probability value between 0 and 1. This probability is used to make binary classification decisions based on a threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b8bbc-2727-4835-b7a6-0060b39d3cc5",
   "metadata": {},
   "source": [
    "# Explain the concept of the cost function in logistic regression\"\n",
    "\n",
    "| **Aspect**               | **Description**                                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|\n",
    "| **Definition**           | The **cost function** (or loss function) measures the performance of the logistic regression model by quantifying the difference between the predicted probabilities and the actual class labels. |\n",
    "| **Function**             | \\[ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\left[y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i))\\right] \\] <br> where: <br> - \\( J(\\theta) \\) is the cost function. <br> - \\( m \\) is the number of training examples. <br> - \\( y_i \\) is the actual label for the \\( i \\)-th example. <br> - \\( h_\\theta(x_i) \\) is the predicted probability for the \\( i \\)-th example, given by the sigmoid function. |\n",
    "| **Purpose**              | The cost function is used to quantify the error between the predicted probabilities and the actual labels. The goal is to minimize this error during model training. |\n",
    "| **Optimization**         | **Gradient Descent** is commonly used to find the optimal parameters (\\(\\theta\\)) that minimize the cost function. The algorithm iteratively updates the parameters to reduce the cost. |\n",
    "| **Interpretation**       | Lower cost values indicate better model performance, with predictions closer to the actual labels. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "The cost function in logistic regression measures the discrepancy between predicted probabilities and actual class labels. It is minimized using optimization techniques like Gradient Descent to improve model accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603f402-1625-4978-848d-52d74192dee6",
   "metadata": {},
   "source": [
    "# How can logistic regression be extended to handle multiclass classification\n",
    "\n",
    "| **Approach**              | **Description**                                                            |\n",
    "|---------------------------|----------------------------------------------------------------------------|\n",
    "| **1. One-vs-Rest (OvR)**  | **One-vs-Rest** (also known as One-vs-All): Train a separate binary logistic regression classifier for each class. Each classifier predicts whether an instance belongs to its class or not. The final prediction is made by choosing the class with the highest probability. |\n",
    "| **2. One-vs-One (OvO)**   | **One-vs-One**: Train a binary classifier for every possible pair of classes. For \\(k\\) classes, this results in \\(\\frac{k(k-1)}{2}\\) classifiers. Each classifier votes for one of the two classes, and the class with the most votes is chosen as the final prediction. |\n",
    "| **3. Softmax Regression** | **Softmax Regression** (also known as Multinomial Logistic Regression): Extends logistic regression by using the softmax function to handle multiple classes directly. The model outputs a probability distribution over all classes, and the class with the highest probability is chosen as the prediction. <br> \\[ p(y = j | \\mathbf{x}; \\mathbf{\\theta}) = \\frac{e^{\\mathbf{\\theta}_j^T \\mathbf{x}}}{\\sum_{k=1}^K e^{\\mathbf{\\theta}_k^T \\mathbf{x}}} \\] |\n",
    "\n",
    "### Summary\n",
    "\n",
    "Logistic Regression can be extended to multiclass classification using One-vs-Rest, One-vs-One, or Softmax Regression. Softmax Regression is the most common method, directly handling multiple classes by providing a probability distribution over all possible classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9d9f3-7c99-4c9c-baa6-a3e5b3bd4ff6",
   "metadata": {},
   "source": [
    "# What is the difference between L1 and L2 regularization in logistic regression\n",
    "\n",
    "| **Aspect**               | **L1 Regularization (Lasso)**                                                | **L2 Regularization (Ridge)**                                                |\n",
    "|--------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|\n",
    "| **Penalty Term**         | **L1 Penalty**: Adds the sum of the absolute values of the coefficients. <br> \\[ \\lambda \\sum_{j} |\\theta_j| \\] | **L2 Penalty**: Adds the sum of the squared values of the coefficients. <br> \\[ \\lambda \\sum_{j} \\theta_j^2 \\] |\n",
    "| **Effect on Coefficients** | **Sparsity**: Can drive some coefficients to exactly zero, effectively performing feature selection. | **Shrinkage**: Shrinks coefficients towards zero but does not necessarily make them exactly zero. |\n",
    "| **Feature Selection**    | **Implicit Feature Selection**: Useful for models with many features where some features may be irrelevant. | **No Implicit Feature Selection**: All features are kept, with coefficients being reduced in magnitude. |\n",
    "| **Computation**          | Can be less stable due to sparsity and may require more careful tuning. | Generally more stable and easier to tune compared to L1 regularization. |\n",
    "| **Interpretability**     | **Higher Interpretability**: Provides simpler models with fewer features due to sparsity. | **Lower Interpretability**: Models include all features, making them potentially more complex. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "L1 regularization (Lasso) adds a penalty based on the absolute values of coefficients, promoting sparsity and feature selection. L2 regularization (Ridge) adds a penalty based on the squared values of coefficients, promoting coefficient shrinkage and stability without performing feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cade396-8aa5-41c4-9650-566af0a94b11",
   "metadata": {},
   "source": [
    "# What is XGBoost and how does it differ from other boosting algorithms\n",
    "\n",
    "| **Aspect**               | **XGBoost**                                                                 | **Other Boosting Algorithms**                                              |\n",
    "|--------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------------------|\n",
    "| **Definition**           | **XGBoost** (Extreme Gradient Boosting) is an optimized gradient boosting algorithm designed for speed and performance. | Other boosting algorithms include AdaBoost, Gradient Boosting, and LightGBM. |\n",
    "| **Boosting Type**        | **Gradient Boosting**: Builds trees sequentially, where each tree corrects the errors of the previous ones. | Similar gradient boosting approach but with different optimizations. |\n",
    "| **Regularization**       | **Built-in Regularization**: Includes L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting. | Many other boosting algorithms do not have built-in regularization or have different regularization techniques. |\n",
    "| **Handling Missing Values** | **Automatic Handling**: Can handle missing values internally without needing imputation. | Other algorithms may require explicit handling of missing values before training. |\n",
    "| **Parallelism**          | **Parallel Processing**: Utilizes parallel processing and optimization to improve training speed. | Some boosting algorithms do not leverage parallel processing as efficiently. |\n",
    "| **Tree Pruning**         | **Post-Pruning**: Uses a depth-first approach to grow trees and prunes them afterward, which can be more efficient. | Other algorithms might use different tree-building and pruning strategies. |\n",
    "| **Scalability**          | **High Scalability**: Designed to work efficiently with large datasets and high-dimensional data. | Some boosting algorithms might be less scalable or slower with large datasets. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "XGBoost is an advanced gradient boosting algorithm known for its speed, performance, and built-in regularization. It differs from other boosting algorithms through its efficient handling of missing values, parallel processing, and scalability, as well as its approach to tree pruning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b8f27-d45f-448d-a994-047881871845",
   "metadata": {},
   "source": [
    "# Explain the concept of boosting in the context of ensemble learning\"\n",
    "\n",
    "| **Aspect**               | **Description**                                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|\n",
    "| **Definition**           | **Boosting** is an ensemble learning technique that combines the outputs of multiple weak learners to create a strong predictive model. |\n",
    "| **Process**              | 1. **Sequential Learning**: Train models sequentially, where each new model focuses on correcting the errors made by previous models. <br> 2. **Weighted Errors**: Assign higher weights to misclassified instances to emphasize them in subsequent models. |\n",
    "| **Weak Learner**         | Typically, a weak learner is a simple model like a decision tree with limited depth. |\n",
    "| **Model Combination**    | Combine the predictions of all weak learners to form a final prediction, often by weighting their contributions based on their performance. |\n",
    "| **Key Algorithms**       | - **AdaBoost**: Adjusts weights based on classification errors of previous models. <br> - **Gradient Boosting**: Fits new models to the residual errors of previous models. <br> - **XGBoost**: An optimized version of gradient boosting with additional features like regularization and parallel processing. |\n",
    "| **Benefits**             | - **Improved Accuracy**: Boosting often leads to higher predictive accuracy compared to individual models. <br> - **Error Reduction**: Focuses on reducing errors of weak learners by iteratively improving model performance. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "Boosting is an ensemble technique that builds models sequentially, with each new model correcting the errors of its predecessors. It combines multiple weak learners to enhance predictive accuracy and reduce errors, employing algorithms like AdaBoost, Gradient Boosting, and XGBoost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ba377-c20c-4b84-8070-607175d896ef",
   "metadata": {},
   "source": [
    "# How does XGBoost handle missing values\n",
    "\n",
    "| **Aspect**               | **Description**                                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|\n",
    "| **Automatic Handling**   | XGBoost can handle missing values internally without requiring explicit imputation. |\n",
    "| **Missing Value Handling** | **Sparsity Aware**: During training, XGBoost learns the best way to split data, even if some values are missing. It does this by exploring potential splits for missing values and determining the optimal way to handle them. |\n",
    "| **Split Finding**        | XGBoost evaluates missing values during the tree-building process. For each split, it considers different ways to handle missing data (e.g., routing missing values to the left or right child node) and chooses the option that improves model performance. |\n",
    "| **Default Directions**   | **Default Directions**: XGBoost assigns default directions (left or right) for missing values in each split. The direction is chosen based on which choice improves the accuracy of the model. |\n",
    "| **Model Training**       | During model training, XGBoost adjusts its approach to accommodate missing values, ensuring that they do not negatively impact the learning process. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "XGBoost handles missing values internally by learning the optimal way to split data with missing values during training. It evaluates potential splits and assigns default directions for missing values to ensure that they do not adversely affect model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd26df-c1a7-4e58-8436-c586ca2b914f",
   "metadata": {},
   "source": [
    "# What are the key hyperparameters in XGBoost and how do they affect model performance\n",
    "\n",
    "| **Hyperparameter**       | **Description**                                                            | **Effect on Model Performance**                                             |\n",
    "|--------------------------|----------------------------------------------------------------------------|------------------------------------------------------------------------------|\n",
    "| **n_estimators**         | Number of boosting rounds or trees to build.                               | More trees can improve performance but may increase training time and risk of overfitting. |\n",
    "| **learning_rate (eta)**  | Step size shrinkage used in update to prevent overfitting.                  | Smaller values can lead to better performance but require more boosting rounds. |\n",
    "| **max_depth**            | Maximum depth of each tree.                                                 | Deeper trees can model more complex relationships but may lead to overfitting. |\n",
    "| **min_child_weight**     | Minimum sum of instance weight (hessian) needed in a child.                 | Higher values prevent overfitting by requiring more samples to create a split. |\n",
    "| **subsample**            | Fraction of samples used for each boosting round.                           | Values between 0.5 and 1.0 can prevent overfitting by randomly sampling training data. |\n",
    "| **colsample_bytree**     | Fraction of features used for each tree.                                    | Helps prevent overfitting by randomly sampling features. |\n",
    "| **gamma**                | Minimum loss reduction required to make a further partition on a leaf node. | Higher values make the algorithm more conservative, reducing the complexity of the model. |\n",
    "| **scale_pos_weight**     | Controls the balance of positive and negative weights.                      | Useful for handling class imbalance by adjusting the weight of positive class samples. |\n",
    "| **lambda (reg_lambda)**  | L2 regularization term on weights.                                           | Helps prevent overfitting by adding a penalty for large coefficients. |\n",
    "| **alpha (reg_alpha)**    | L1 regularization term on weights.                                           | Encourages sparsity by adding a penalty for non-zero coefficients. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "Key hyperparameters in XGBoost include the number of trees (`n_estimators`), learning rate (`learning_rate`), tree depth (`max_depth`), and regularization terms (`lambda` and `alpha`). These parameters control model complexity, prevent overfitting, and can significantly affect the model's performance. Adjusting them appropriately helps in achieving a balance between bias and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a1691-0d70-4802-a766-f9794b43a894",
   "metadata": {},
   "source": [
    "# Describe the process of gradient boosting in XGBoost\"\n",
    "\n",
    "| **Step**                 | **Description**                                                            |\n",
    "|--------------------------|----------------------------------------------------------------------------|\n",
    "| **1. Initialize Model**  | Start with an initial model (often a simple model, such as a mean prediction for regression or class probability for classification). |\n",
    "| **2. Compute Residuals** | Calculate the residuals or errors from the initial model. These represent the difference between the predicted values and the actual values. |\n",
    "| **3. Train Weak Learner**| Fit a new weak learner (usually a decision tree) to the residuals. This learner tries to predict the errors made by the previous model. |\n",
    "| **4. Update Model**      | Update the model by adding the new weak learner to the existing model. The learning rate (`eta`) controls how much contribution the new learner makes to the model. |\n",
    "| **5. Iterate**           | Repeat steps 2-4 for a specified number of boosting rounds or until the residuals are minimized. Each iteration refines the model by focusing on the errors of previous iterations. |\n",
    "| **6. Regularize**        | Apply regularization (L1 and L2) during training to prevent overfitting and improve model generalization. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "In XGBoost, the gradient boosting process involves initializing a model, computing residuals, training a weak learner to predict these residuals, updating the model with the new learner, and iterating this process while applying regularization to control overfitting. Each iteration refines the model to better predict the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684af095-968b-4c4d-9213-699fc4367280",
   "metadata": {},
   "source": [
    "# What are the advantages and disadvantages of using XGBoost?\n",
    "\n",
    "| **Aspect**                | **Advantages**                                                            | **Disadvantages**                                                           |\n",
    "|---------------------------|----------------------------------------------------------------------------|------------------------------------------------------------------------------|\n",
    "| **Performance**           | **High Performance**: Often provides superior predictive accuracy compared to other algorithms. | **Complexity**: Can be complex to tune and require careful parameter optimization. |\n",
    "| **Speed**                 | **Fast Training**: Optimized for speed and efficiency, with parallel and distributed computing capabilities. | **Resource Intensive**: Can be resource-intensive, requiring substantial memory and computation, especially with large datasets. |\n",
    "| **Handling Missing Values** | **Automatic Handling**: Can handle missing values internally without requiring imputation. | **Overfitting Risk**: May overfit if not properly tuned, especially with complex models and inadequate regularization. |\n",
    "| **Flexibility**           | **Versatile**: Can be used for regression, classification, and ranking tasks, and supports various objective functions. | **Interpretability**: The model can be less interpretable compared to simpler models like linear regression or decision trees. |\n",
    "| **Regularization**        | **Built-in Regularization**: Includes L1 and L2 regularization to prevent overfitting and enhance model generalization. | **Parameter Tuning**: Requires careful tuning of hyperparameters to achieve optimal performance. |\n",
    "| **Scalability**           | **Scalable**: Designed to work efficiently with large datasets and high-dimensional data. | **Implementation Complexity**: More complex to implement and understand compared to simpler algorithms. |\n",
    "\n",
    "### Summary\n",
    "\n",
    "XGBoost is a high-performance algorithm known for its speed, ability to handle missing values, and built-in regularization. However, it can be complex to tune, resource-intensive, and less interpretable, with risks of overfitting if not properly managed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0a6f4-9301-4214-b85d-f5a179a68bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
